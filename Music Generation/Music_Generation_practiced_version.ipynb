{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Generation practiced version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3MEG1YHqy-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import *\n",
        "from music21 import *\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqYwimyBeS69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_directory = \"./drive/My Drive/Data/\"\n",
        "data_file = \"Data_Tunes.txt\"\n",
        "charIndex_json = \"char_to_index.json\"\n",
        "BATCH_SIZE = 16\n",
        "SEQ_LENGTH = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac4PKDrDeZZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to remove Part-1 of  and append a start token i.e 'Z' for each tune in the dataset\n",
        "\n",
        "def preprocess(data):\n",
        "  list1=list(data)\n",
        "  list2=['\\n','\\n','\\n']\n",
        "  ignore=['X','T','M','S','K','P']\n",
        "  i=0\n",
        "  #to remove Part1:\n",
        "  while(i<len(list1)):\n",
        "    if(((list1[i] in ignore) and (list1[i+1]==\":\"))or list1[i]=='%' ):\n",
        "      del list2[-1]\n",
        "      while(list1[i]!='\\n'):\n",
        "        i=i+1\n",
        "    list2.append(list1[i])\n",
        "    i=i+1\n",
        "  i=0\n",
        "  #to append 'Z'(start token)\n",
        "  preprocess_data=[]\n",
        "  while(i<len(list2)):\n",
        "    if(list2[i]=='\\n'and list2[i+1]=='\\n' and list2[i+2]=='\\n'):\n",
        "      preprocess_data.append('Z')\n",
        "      i=i+3\n",
        "    else:\n",
        "      preprocess_data.append(list2[i])\n",
        "      i=i+1\n",
        "  return preprocess_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmU1neYUecy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create char_to_index and index_to_char dictionaries so as to map each character to an index and vice versa.\n",
        "# Returns all_characters_as_indices i.e an array containing all characters of the dataset replaced with their corresponding indices as per the vocabulary.\n",
        "# Also returns num_unique_chars i.e an integer equal to number of unique characters in the data.\n",
        "\n",
        "def read_data(preprocess_data):\n",
        "  char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(preprocess_data))))}\n",
        "\n",
        "    \n",
        "  with open(os.path.join(data_directory, charIndex_json), mode = \"w\") as f:\n",
        "        json.dump(char_to_index, f)\n",
        "        \n",
        "  index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
        "  num_unique_chars = len(char_to_index)\n",
        "  all_characters_as_indices = np.asarray([char_to_index[c] for c in preprocess_data], dtype = np.int32)\n",
        "  return all_characters_as_indices,num_unique_chars\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgfKMid9ee0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function which returns X and Y which will be used as input and target output for training the model.\n",
        "\n",
        "def input_output(all_chars_as_indices,num_unique_chars):\n",
        "    total_length = all_chars_as_indices.shape[0]\n",
        "    num_examples=int(total_length/SEQ_LENGTH)\n",
        "    X=np.zeros((num_examples,SEQ_LENGTH))\n",
        "    Y=np.zeros((num_examples,SEQ_LENGTH,num_unique_chars))\n",
        "    for i in range(num_examples):\n",
        "      for j in range(SEQ_LENGTH):\n",
        "        X[i,j]=all_chars_as_indices[i*SEQ_LENGTH+j]\n",
        "        Y[i,j,all_chars_as_indices[i*SEQ_LENGTH+j+1]]=1\n",
        "    return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9_MRT00eh3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to build the training model\n",
        "\n",
        "def build_model( seq_length, num_unique_chars):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim = num_unique_chars, output_dim = 512, input_shape = (seq_length,))) \n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(256, return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    \n",
        "    model.add(TimeDistributed(Dense(num_unique_chars)))\n",
        "\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hueCJnD_ekJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function which builds model for generating music sequences.\n",
        "\n",
        "def make_model(num_unique_chars):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim = num_unique_chars, output_dim = 512, batch_input_shape = (1, 1))) \n",
        "  \n",
        "# stateful: If True, the last state for each sample at index i in a batch will be used \n",
        "# as initial state for the sample of index i in the following batch.\n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(256,return_sequences=True, stateful = True)) \n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add((Dense(num_unique_chars)))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un5WWLWgf4zh",
        "colab_type": "text"
      },
      "source": [
        "Here I have changed the file path as \"weights.80.hdfs\" is loaded to content file as shown so I have used direct name instead of path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX2cihwtem5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function which generates music sequences of length=gen_seq_length.\n",
        "def generate_sequence(gen_seq_length):\n",
        "    with open(os.path.join(data_directory, charIndex_json)) as f:\n",
        "        char_to_index = json.load(f)\n",
        "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
        "    num_unique_chars = len(index_to_char)\n",
        "    \n",
        "    model = make_model(num_unique_chars)\n",
        "    model.load_weights(\"weights.80.hdf5\")\n",
        "     \n",
        "    sequence_index = [char_to_index['Z']]\n",
        "\n",
        "    for _ in range(gen_seq_length):\n",
        "        batch = np.zeros((1, 1))\n",
        "        batch[0, 0] = sequence_index[-1]\n",
        "        \n",
        "        predicted_probs = model.predict_on_batch(batch).ravel()\n",
        "        sample = np.random.choice(range(num_unique_chars), size = 1, p = predicted_probs)\n",
        "        \n",
        "        \n",
        "        sequence_index.append(sample[0])\n",
        "    \n",
        "        \n",
        "    \n",
        "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
        "    seq='M:6/8\\n'+str(seq)\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNyg4fz5epfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to create a midi file given a music sequence in abc notation.\n",
        "def convert_to_midi(abc):\n",
        "    c = converter.subConverters.ConverterABC()\n",
        "    c.registerOutputExtensions = (\"midi\", )\n",
        "    c.parseData(abc)\n",
        "    s = c.stream\n",
        "    s.write('midi', fp='demos1.mid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9dnMgwvermV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7431322f-640f-498f-916d-a0872102d87d"
      },
      "source": [
        "file = open(os.path.join(data_directory, data_file), mode = 'r')\n",
        "data = file.read()\n",
        "file.close()\n",
        "preprocess_data=preprocess(data)\n",
        "all_characters_as_indices,num_unique_chars=read_data(preprocess_data)\n",
        "X,Y=input_output(all_characters_as_indices,num_unique_chars)\n",
        "print(\"length of preprocess_data-{}\".format(len(preprocess_data)))\n",
        "print(\"vocab_size={}\".format(num_unique_chars))\n",
        "print(\"all_characters={}\".format(all_characters_as_indices))\n",
        "print(\"length of all_characters-{}\".format(len(all_characters_as_indices)))\n",
        "print(\"shape of X={}\".format(X.shape))\n",
        "print(\"shape of Y={}\".format(Y.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of preprocess_data-116963\n",
            "vocab_size=59\n",
            "all_characters=[33 44 57 ... 15 20 57]\n",
            "length of all_characters-116963\n",
            "shape of X=(1827, 64)\n",
            "shape of Y=(1827, 64, 59)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D35txGq-etLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25ba0d74-5c0d-4223-cbf0-4a1276960590"
      },
      "source": [
        "model=build_model(SEQ_LENGTH,num_unique_chars)\n",
        "model.summary()\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "checkpoint=ModelCheckpoint(filepath='weights.{epoch:02d}.hdf5',monitor='loss',save_best_only=True,save_weights_only=True,period=1)\n",
        "model.fit(X,Y,batch_size=16,epochs=80,callbacks=[checkpoint])\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 64, 512)           30208     \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 64, 256)           787456    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 64, 256)           525312    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 64, 256)           525312    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 64, 256)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 64, 59)            15163     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64, 59)            0         \n",
            "=================================================================\n",
            "Total params: 1,883,451\n",
            "Trainable params: 1,883,451\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 3.0864 - accuracy: 0.1875\n",
            "Epoch 2/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 2.3331 - accuracy: 0.3489\n",
            "Epoch 3/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 1.9140 - accuracy: 0.4378\n",
            "Epoch 4/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 1.7730 - accuracy: 0.4623\n",
            "Epoch 5/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 1.6427 - accuracy: 0.4854\n",
            "Epoch 6/80\n",
            "1827/1827 [==============================] - 108s 59ms/step - loss: 1.5574 - accuracy: 0.5032\n",
            "Epoch 7/80\n",
            "1827/1827 [==============================] - 109s 60ms/step - loss: 1.4970 - accuracy: 0.5206\n",
            "Epoch 8/80\n",
            "1827/1827 [==============================] - 117s 64ms/step - loss: 1.4484 - accuracy: 0.5307\n",
            "Epoch 9/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 1.4084 - accuracy: 0.5415\n",
            "Epoch 10/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 1.3740 - accuracy: 0.5514\n",
            "Epoch 11/80\n",
            "1827/1827 [==============================] - 108s 59ms/step - loss: 1.3433 - accuracy: 0.5592\n",
            "Epoch 12/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 1.3149 - accuracy: 0.5684\n",
            "Epoch 13/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 1.2840 - accuracy: 0.5786\n",
            "Epoch 14/80\n",
            "1827/1827 [==============================] - 115s 63ms/step - loss: 1.2584 - accuracy: 0.5855\n",
            "Epoch 15/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 1.2342 - accuracy: 0.5933\n",
            "Epoch 16/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 1.2106 - accuracy: 0.6001\n",
            "Epoch 17/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 1.1901 - accuracy: 0.6084\n",
            "Epoch 18/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 1.1686 - accuracy: 0.6142\n",
            "Epoch 19/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 1.1496 - accuracy: 0.6196\n",
            "Epoch 20/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 1.1281 - accuracy: 0.6264\n",
            "Epoch 21/80\n",
            "1827/1827 [==============================] - 109s 59ms/step - loss: 1.1110 - accuracy: 0.6325\n",
            "Epoch 22/80\n",
            "1827/1827 [==============================] - 109s 60ms/step - loss: 1.0932 - accuracy: 0.6368\n",
            "Epoch 23/80\n",
            "1827/1827 [==============================] - 110s 60ms/step - loss: 1.0714 - accuracy: 0.6438\n",
            "Epoch 24/80\n",
            "1827/1827 [==============================] - 110s 60ms/step - loss: 1.0564 - accuracy: 0.6485\n",
            "Epoch 25/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 1.0408 - accuracy: 0.6548\n",
            "Epoch 26/80\n",
            "1827/1827 [==============================] - 108s 59ms/step - loss: 1.0230 - accuracy: 0.6597\n",
            "Epoch 27/80\n",
            "1827/1827 [==============================] - 108s 59ms/step - loss: 1.0053 - accuracy: 0.6650\n",
            "Epoch 28/80\n",
            "1827/1827 [==============================] - 108s 59ms/step - loss: 0.9922 - accuracy: 0.6700\n",
            "Epoch 29/80\n",
            "1827/1827 [==============================] - 109s 60ms/step - loss: 0.9749 - accuracy: 0.6741\n",
            "Epoch 30/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 0.9601 - accuracy: 0.6801\n",
            "Epoch 31/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 0.9439 - accuracy: 0.6852\n",
            "Epoch 32/80\n",
            "1827/1827 [==============================] - 110s 60ms/step - loss: 0.9283 - accuracy: 0.6902\n",
            "Epoch 33/80\n",
            "1827/1827 [==============================] - 110s 60ms/step - loss: 0.9146 - accuracy: 0.6947\n",
            "Epoch 34/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 0.8973 - accuracy: 0.7004\n",
            "Epoch 35/80\n",
            "1827/1827 [==============================] - 109s 59ms/step - loss: 0.8846 - accuracy: 0.7039\n",
            "Epoch 36/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 0.8696 - accuracy: 0.7096\n",
            "Epoch 37/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 0.8579 - accuracy: 0.7129\n",
            "Epoch 38/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 0.8395 - accuracy: 0.7192\n",
            "Epoch 39/80\n",
            "1827/1827 [==============================] - 110s 60ms/step - loss: 0.8228 - accuracy: 0.7241\n",
            "Epoch 40/80\n",
            "1827/1827 [==============================] - 106s 58ms/step - loss: 0.8115 - accuracy: 0.7278\n",
            "Epoch 41/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 0.7974 - accuracy: 0.7334\n",
            "Epoch 42/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 0.7855 - accuracy: 0.7361\n",
            "Epoch 43/80\n",
            "1827/1827 [==============================] - 110s 60ms/step - loss: 0.7719 - accuracy: 0.7401\n",
            "Epoch 44/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 0.7572 - accuracy: 0.7447\n",
            "Epoch 45/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 0.7429 - accuracy: 0.7492\n",
            "Epoch 46/80\n",
            "1827/1827 [==============================] - 111s 61ms/step - loss: 0.7320 - accuracy: 0.7533\n",
            "Epoch 47/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 0.7192 - accuracy: 0.7580\n",
            "Epoch 48/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 0.7063 - accuracy: 0.7621\n",
            "Epoch 49/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 0.6931 - accuracy: 0.7668\n",
            "Epoch 50/80\n",
            "1827/1827 [==============================] - 112s 61ms/step - loss: 0.6853 - accuracy: 0.7682\n",
            "Epoch 51/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 0.6718 - accuracy: 0.7732\n",
            "Epoch 52/80\n",
            "1827/1827 [==============================] - 117s 64ms/step - loss: 0.6575 - accuracy: 0.7774\n",
            "Epoch 53/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 0.6488 - accuracy: 0.7812\n",
            "Epoch 54/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 0.6352 - accuracy: 0.7853\n",
            "Epoch 55/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 0.6223 - accuracy: 0.7885\n",
            "Epoch 56/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 0.6112 - accuracy: 0.7927\n",
            "Epoch 57/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 0.6007 - accuracy: 0.7974\n",
            "Epoch 58/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 0.5980 - accuracy: 0.7984\n",
            "Epoch 59/80\n",
            "1827/1827 [==============================] - 113s 62ms/step - loss: 0.5853 - accuracy: 0.8021\n",
            "Epoch 60/80\n",
            "1827/1827 [==============================] - 114s 63ms/step - loss: 0.5713 - accuracy: 0.8057\n",
            "Epoch 61/80\n",
            "1827/1827 [==============================] - 114s 63ms/step - loss: 0.5632 - accuracy: 0.8081\n",
            "Epoch 62/80\n",
            "1827/1827 [==============================] - 114s 63ms/step - loss: 0.5515 - accuracy: 0.8116\n",
            "Epoch 63/80\n",
            "1827/1827 [==============================] - 121s 66ms/step - loss: 0.5488 - accuracy: 0.8134\n",
            "Epoch 64/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 0.5378 - accuracy: 0.8168\n",
            "Epoch 65/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 0.5289 - accuracy: 0.8200\n",
            "Epoch 66/80\n",
            "1827/1827 [==============================] - 114s 63ms/step - loss: 0.5193 - accuracy: 0.8228\n",
            "Epoch 67/80\n",
            "1827/1827 [==============================] - 115s 63ms/step - loss: 0.5133 - accuracy: 0.8235\n",
            "Epoch 68/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 0.5032 - accuracy: 0.8283\n",
            "Epoch 69/80\n",
            "1827/1827 [==============================] - 115s 63ms/step - loss: 0.4995 - accuracy: 0.8290\n",
            "Epoch 70/80\n",
            "1827/1827 [==============================] - 114s 63ms/step - loss: 0.4875 - accuracy: 0.8326\n",
            "Epoch 71/80\n",
            "1827/1827 [==============================] - 114s 62ms/step - loss: 0.4805 - accuracy: 0.8356\n",
            "Epoch 72/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 0.4723 - accuracy: 0.8388\n",
            "Epoch 73/80\n",
            "1827/1827 [==============================] - 120s 66ms/step - loss: 0.4691 - accuracy: 0.8393\n",
            "Epoch 74/80\n",
            "1827/1827 [==============================] - 116s 63ms/step - loss: 0.4585 - accuracy: 0.8429\n",
            "Epoch 75/80\n",
            "1827/1827 [==============================] - 116s 63ms/step - loss: 0.4560 - accuracy: 0.8442\n",
            "Epoch 76/80\n",
            "1827/1827 [==============================] - 115s 63ms/step - loss: 0.4464 - accuracy: 0.8459\n",
            "Epoch 77/80\n",
            "1827/1827 [==============================] - 116s 64ms/step - loss: 0.4426 - accuracy: 0.8468\n",
            "Epoch 78/80\n",
            "1827/1827 [==============================] - 118s 65ms/step - loss: 0.4377 - accuracy: 0.8501\n",
            "Epoch 79/80\n",
            "1827/1827 [==============================] - 124s 68ms/step - loss: 0.4299 - accuracy: 0.8519\n",
            "Epoch 80/80\n",
            "1827/1827 [==============================] - 118s 65ms/step - loss: 0.4256 - accuracy: 0.8532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fbc35e0c940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DifQZAEFewo9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "425e1e94-96a4-4a4d-a468-7fb297aee473"
      },
      "source": [
        "music = generate_sequence(194)\n",
        "print(\"\\nMUSIC SEQUENCE GENERATED: \\n{}\".format(music))\n",
        "convert_to_midi(music)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MUSIC SEQUENCE GENERATED: \n",
            "M:6/8\n",
            "ZD|\"G\"G2A B2c|\"G\"dge dBG|\"Am\"E3 A2G|\"D7\"F2D F2E|\"C\"E3 \"D7\"A2B|\"G\"A3 G2F|\"C\"E2E EFG|\n",
            "\"G\"D2D DGA|\"C\"B2A \"D\"DFA|\"G\"B2A \"A7\"GAB|\"D\"A2F D2E|\"Em\"EFG \"A7\"EFG|\"D\"F2A \"A7\"G2F|\"Em\"E2F G2A|\"Em\"Bcd \"A7\"edc|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1nUhgshib-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}