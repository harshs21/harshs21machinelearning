{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "architectures.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfEidpr7HvD",
        "colab_type": "text"
      },
      "source": [
        "# Modern image classification CNN architectures\n",
        "\n",
        "----\n",
        "\n",
        "#### 2012: data, GPU, CUDA\n",
        "\n",
        "Convolutional networks have been around since the 80's and their first breakthrough in computer vision came in 2012. \n",
        "The main reson behind this success was a large dataset, powerful hardware (GPU) and the software to actually train a large network on lots of data on a GPU. And one trick: ReLU. Interestingly not much has changed in the network architectures until 2012, the Alexnet is based in the same principles as LeNet.\n",
        "\n",
        "#### 2012-2017: architectures, tricks, (stronger GPUs)\n",
        "\n",
        "Since 2012, errors continued to decrease from 15% to 2.2% today.  These results are mostly the result of improvements of the models: architectures and other tricks. \n",
        "The new results are achieved using the same dataset.\n",
        "A small part of the improvement can be attributed to stronger hardware.\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        "\n",
        "In this notebook we will try to cover the most important concepts and improvements in convolution neural network design. Obviously this task is huge, and we will just cover some selected concepts without going into much details. In order to understand the concept more deeply we will refer to the original papers introducing the concepts.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D_sWYIy7HvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdXjS8Vj7HvR",
        "colab_type": "text"
      },
      "source": [
        "## LeNet\n",
        "\n",
        "The original model from [LeCun 1998](http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf), used to classify hand-written digits.\n",
        "\n",
        "\n",
        "![lenet5](http://deeplearning.net/tutorial/_images/mylenet.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJTlnq8C7HvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lenet = Sequential()\n",
        "\n",
        "# block 1\n",
        "lenet.add(Conv2D(4, kernel_size=(5, 5), activation='tanh',\n",
        "                 input_shape=(28,28,1)))\n",
        "lenet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# block 2\n",
        "lenet.add(Conv2D(6, (5, 5), activation='tanh'))\n",
        "lenet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# dense classifier\n",
        "lenet.add(Flatten())\n",
        "lenet.add(Dense(500, activation='tanh'))\n",
        "lenet.add(Dense(10, activation='softmax'))\n",
        "\n",
        "SVG(model_to_dot(lenet,show_shapes=True, show_layer_names=False\n",
        "                ).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPcHxZkA7Hvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lenet.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I20Um3q97Hvi",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "\n",
        "## Let's skip 2012 and pick up progress in 2014 \n",
        "\n",
        "\n",
        "\n",
        "### Vgg16\n",
        "\n",
        "- 2nd place in ILSVRC 2014\n",
        "- the best single model of the competition ( more tricks in the winner )\n",
        "- [arxiv paper](https://arxiv.org/abs/1409.1556)\n",
        "\n",
        "A few architectrural changes compared to LeNet.\n",
        "\n",
        "* ReLU non-linearity instead of tanh or sigmoid\n",
        "* move to 3x3 conv ( and 2 convolutions per blocks instead of 1 ) ('deeper')\n",
        "* larger images -> repeat blocks multiple times to achieve large FOV for last conv untis  ('deeper')\n",
        "* richer/more data: more filters ('wider' model)\n",
        "* And a regularization layer: Dropout ([link to orignal paper](www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
        "    * during training randomly knock out a fraction of neurons (0 output)\n",
        "    * during testing switch all on ( multiply outputs with the dropout probabilty )\n",
        "    * the results is something like an 'ensemble' of slightly different networks\n",
        "    * it's popularity has declined but still used in the best \"inception\" networks \n",
        "\n",
        "\n",
        "( Note the keras functional API, functions operate over tensors! )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "imjYE8aC7Hvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_input = Input(shape=(224,224,3),name='input')\n",
        "\n",
        "# Block 1\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "# Block 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "# Block 3\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "# Block 4\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "# Block 5\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "# Classification block\n",
        "x = Flatten(name='flatten')(x)\n",
        "x = Dropout(0.5,name='Dropout1')(x)\n",
        "x = Dense(4096, activation='relu', name='fc1')(x)\n",
        "x = Dropout(0.5,name='Dropout2')(x)\n",
        "x = Dense(4096, activation='relu', name='fc2')(x)\n",
        "x = Dense(1000, activation='softmax', name='predictions')(x)\n",
        "\n",
        "vgg16 = Model(inputs=img_input, outputs=x)\n",
        "SVG(model_to_dot(vgg16,show_shapes=True, show_layer_names=True\n",
        "                ).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_iMyXUM7Hvq",
        "colab_type": "text"
      },
      "source": [
        "##### Parameters:\n",
        "\n",
        "(Note that out of 138M params 103M is in the first dense layer! )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCVVZHJd7Hvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg16.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slD3K6S7Hv0",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "## Dropout\n",
        "\n",
        "\n",
        "From the original article:\n",
        "\n",
        "*\"The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much.\"*\n",
        "\n",
        "*\"During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights.\"*\n",
        "\n",
        "*\"This significantly reduces overfitting and gives major improvements over other regularization methods. \"*\n",
        "\n",
        "\n",
        "##### Practical potins:\n",
        "* overfitting becomes a lot lot harder\n",
        "* usually applied to the final dense layers (if there is any)\n",
        "* slows down training\n",
        "\n",
        "\n",
        "In many convolutional nets it is not used anymore, altough one of the most succesful arctiectures, [inception_v4](https://arxiv.org/abs/1602.07261) still uses it.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Figure 1 from the original paper:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGEwvDw77Hv1",
        "colab_type": "text"
      },
      "source": [
        "![Dropout](https://cdn-images-1.medium.com/max/2000/1*iWQzxhVlvadk6VAJjsgXgg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAyBLTkn7Hv2",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## GoogLeNet\n",
        "\n",
        "\n",
        "Googlenet: Winner of the 2014 ILSVRC, the architecture has been developed ever since, and it is one the state of the art computer vision architectures.\n",
        "\n",
        "[Original article](https://arxiv.org/abs/1409.4842)\n",
        "\n",
        "Vgg16 was simple, inception is full of tricks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldR_yG9v7Hv4",
        "colab_type": "text"
      },
      "source": [
        "### Inception module\n",
        "\n",
        "\n",
        "* Vgg16: simple bulding block of 2 3x3 convolution followed by a maxpooling layer.\n",
        "* Googlenet: Inception, multi path buliding block.\n",
        "\n",
        "* Multi size convolutions 1x1, 3x3, 5x5, some explanation from the article: \n",
        "    * *\"one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches, and there will be a decreasing number of patches over larger and larger regions\"*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CNhk7Rj7Hv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(14, 14,512),name='input')\n",
        "\n",
        "tower_0 = Conv2D(128, (1, 1), padding='same', activation='relu', name='1x1_1')(input_img)\n",
        "\n",
        "tower_1 = Conv2D(128, (1, 1), padding='same', activation='relu', name='1x1_3')(input_img)\n",
        "tower_1 = Conv2D(256, (3, 3), padding='same', activation='relu', name='3x3')(tower_1)\n",
        "\n",
        "tower_2 = Conv2D(24, (1, 1), padding='same', activation='relu', name='1x1_5')(input_img)\n",
        "tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu', name='5x5')(tower_2)\n",
        "\n",
        "tower_3 = MaxPooling2D((3, 3), strides=(1, 1), padding='same', name='MaxPool')(input_img)\n",
        "tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu', name='Conv')(tower_3)\n",
        "\n",
        "output = keras.layers.concatenate([tower_0, tower_1, tower_2, tower_3],\n",
        "                                  name ='concat', axis=3)\n",
        "\n",
        "inception = Model(inputs=input_img, outputs=output)\n",
        "SVG(model_to_dot(inception,show_shapes=False, show_layer_names=True\n",
        "                ).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA3FH9qr7Hv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOa1n45D7HwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SVG(model_to_dot(inception,show_shapes=True, show_layer_names=True\n",
        "                ).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKK-SGsv7HwE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## Bottlenecks\n",
        "\n",
        "Apart from multiple paths with different convolutional kernel sizes, inception modules feature a 1x1 convolutional **compression** step before 3x3 or 5x5 convolutions.\n",
        "\n",
        "The main idea is that $k \\times k$ convolutional layers use $(k^2+1) \\times nfilt_{prev} \\times nfilt_{next}$ parameters, which starts to be very heavy at the later stages of the network where the spatial dimensions are reduced, but the number of filters reaches 500 or 1000. \n",
        "\n",
        "\n",
        "It's possible to keep the computational burden and the number of parameters low, and keep the models representational power similar. Before the convolution with filter size 3 or 5, we can compress the representation with 1x1 'convolution' into a temporary stage. \n",
        "\n",
        "In the case of the 5x5 convolutions, the inception module above agressively compresses the 512 filters into 24 filters and then computes the 5x5 convolution. The number of filter parameters thus becomes: $ 2 \\times 512 \\times 24  + (5 \\times 5 + 1 ) \\times 24 \\times 64  = 64,512 $ instead of a naive 5x5 convoltuion with 512 input and 64 output filter which has $(5 \\times 5 + 1 )  \\times 512 \\times 64  = 851,968 $ parameters.\n",
        "\n",
        "The 3x3 branch uses a similar but less aggressive stategy to reduce parameter size to 458,752 from 1,310,720.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfdkVY3P7HwF",
        "colab_type": "text"
      },
      "source": [
        "A 3x3 convolution from 512 filters to 512 filters without bottleneck costs 2.36M params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sMKaWPv7HwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(None, None,512),name='input')\n",
        "\n",
        "# first block with no bottleneck to demonstrate\n",
        "x = Conv2D(512, (3, 3), padding='same', activation='relu', name='3x3_convolution')(input_img)\n",
        "\n",
        "no_bottleneck = Model(inputs=input_img, outputs=x)\n",
        "no_bottleneck.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YjXTs4f7HwJ",
        "colab_type": "text"
      },
      "source": [
        "A 3x3 convolution with bottleneck mapping from 512 filters to 512 filters costs only 0.59M params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrEN2Ycp7HwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(None, None,512),name='input')\n",
        "\n",
        "# second  block with bottleneck to demonstrate\n",
        "x = Conv2D(128, (1, 1), padding='same', activation='relu', name='1x1_conv_compression')(input_img)\n",
        "x = Conv2D(512, (3, 3), padding='same', activation='relu', name='3x3_conv')(x)\n",
        "\n",
        "bottleneck = Model(inputs=input_img, outputs=x)\n",
        "bottleneck.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beObSz6J7HwO",
        "colab_type": "text"
      },
      "source": [
        "Obviously a compression step like this might hurt the representational power of the model, but the gains turn out to be larger than the loss. \n",
        "\n",
        "In the end a layer with more filters and a bottleneck has larger representational power than a layer with the same number of parameters but no bottleneck.\n",
        "\n",
        "----\n",
        "\n",
        "Example for no bottleneck with similar number parameters:\n",
        "\n",
        "A layer without bottleneck but 0.59M params can have 256 filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7IAhiP-7HwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(None, None,256),name='input')\n",
        "\n",
        "# first block with no bottleneck to demonstrate\n",
        "x = Conv2D(256, (3, 3), padding='same', activation='relu', name='3x3_convolution')(input_img)\n",
        "\n",
        "no_bottleneck = Model(inputs=input_img, outputs=x)\n",
        "no_bottleneck.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dehktw7HwT",
        "colab_type": "text"
      },
      "source": [
        "##### With decompression\n",
        "\n",
        "Also reducing the output of the 3x3 convolution, and decompressing it with 1x1 further reduces computation/parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-VGUyzY7HwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_img = Input(shape=(None, None,512),name='input')\n",
        "\n",
        "# second  block with bottleneck to demonstrate\n",
        "x = Conv2D(128, (1, 1), padding='same', activation='relu', name='1x1_conv_compression')(input_img)\n",
        "x = Conv2D(128, (3, 3), padding='same', activation='relu', name='3x3_conv')(x)\n",
        "x = Conv2D(512, (1, 1), padding='same', activation='relu', name='1x1_decomp')(x)\n",
        "\n",
        "\n",
        "bottleneck = Model(inputs=input_img, outputs=x)\n",
        "bottleneck.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLl9Z_tZ7HwX",
        "colab_type": "text"
      },
      "source": [
        "#### Messages:\n",
        "\n",
        "* Bottlenecks help improve the representational power of models while keeping the computational/paramater budget constant\n",
        "* [Adding bottlenecks to the vgg16/19](https://pjreddie.com/darknet/imagenet/) model enables it to reduce single run top5 error from 10% to 6.5%. (also remove dense layers, dropout and add batchnorm). The model is called Darknet19.\n",
        "\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbQAtv5e7HwY",
        "colab_type": "text"
      },
      "source": [
        "### Auxiliary classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W58Hhyx87HwZ",
        "colab_type": "text"
      },
      "source": [
        "Another trick in googlenet, was to add extra output nodes in the middle of the network to allow lower layers to have contact with the output, to help \"gradient flow\".\n",
        "\n",
        "These outputs were not used during the final predictions they are only used to give feedback to parameters at lower stages.\n",
        "\n",
        "---\n",
        "\n",
        "An example of auxiliary classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTVi1--T7Hwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_input = Input(shape=(32,32,3),name='input')\n",
        "\n",
        "# Block 1\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
        "\n",
        "# Aux Classification block\n",
        "xa1 = Flatten(name='flatten_aux1')(x)\n",
        "xa1 = Dense(1000, activation='softmax', name='predictions_aux1')(xa1)\n",
        "\n",
        "# Block 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "\n",
        "\n",
        "# Aux Classification block\n",
        "xa2 = Flatten(name='flatten_aux2')(x)\n",
        "xa2 = Dense(1000, activation='softmax', name='predictions_aux2')(xa2)\n",
        "\n",
        "# Block 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "\n",
        "# Classification block\n",
        "x = Flatten(name='flatten_main')(x)\n",
        "x = Dense(1000, activation='softmax', name='predictions_main')(x)\n",
        "\n",
        "\n",
        "model_w_aux = Model(inputs=img_input, outputs=[x,xa1,xa2])\n",
        "SVG(model_to_dot(model_w_aux,show_shapes=False, show_layer_names=True\n",
        "                ).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GLxsZce7Hwc",
        "colab_type": "text"
      },
      "source": [
        "Auxuliary classifiers are still used in (e.g.: inception_v3) but their role was mostly replaced by \"skip connections\" and \"residual\" networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTVp2uFu7Hwd",
        "colab_type": "text"
      },
      "source": [
        "## Resnets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRUPyvX87Hwd",
        "colab_type": "text"
      },
      "source": [
        "Surprisingly stacking more layers to the vgg16 model, does not help. It turned out that simple and too deep networks just can not be trained. A 56 layer deep basic convolutional model has higher error rate both on the test data and the traning data. Even though they have more parameters, are not even able to overfit the data.\n",
        "\n",
        "\n",
        "Figure 1 from He et al 2015:\n",
        "![src](https://wiki.tum.de/download/attachments/22578294/Figure%201.bmp?version=1&modificationDate=1485208088253&api=v2)\n",
        "\n",
        "This results seems to indicated that there is some problem with the optamization scheme used to train the model. Googlenet try to overcome the problem with auxiliary classifiers, others introduced \"Residual networks\" or skip connections in order to train deeper models.\n",
        "\n",
        "This concept enabled the MSRA team to win the ILSVRC in 2015.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D6NXE1I7Hwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Input(shape=(None,None,64))\n",
        "# 3x3 conv with 3 output channels (same as input channels)\n",
        "y = Conv2D(64, (3, 3), padding='same')(x)\n",
        "# this returns x + y.\n",
        "z = keras.layers.add([x, y])\n",
        "\n",
        "model = Model(inputs=x, outputs=z)\n",
        "SVG(model_to_dot(model,show_shapes=False, show_layer_names=False).create(prog='dot', format='svg',))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhYxDFSY7Hwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Input(shape=(None,None,64))\n",
        "# 3x3 conv with 3 output channels (same as input channels)\n",
        "y = Conv2D(64, (3, 3), padding='same')(x)\n",
        "# this returns x + y.\n",
        "z = keras.layers.add([x, y])\n",
        "\n",
        "# 3x3 conv with 3 output channels (same as input channels)\n",
        "a = Conv2D(64, (3, 3), padding='same')(z)\n",
        "# this returns x + y.\n",
        "out = keras.layers.add([z, a])\n",
        "\n",
        "model = Model(inputs=x, outputs=out)\n",
        "SVG(model_to_dot(model,show_shapes=False, show_layer_names=False).create(prog='dot', format='svg',))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gA56NOA7Hwi",
        "colab_type": "text"
      },
      "source": [
        "#### Interpretation 1:\n",
        "\n",
        "Their original interpretation: \n",
        "\n",
        "Learnig the small difference instead of learning identity + a small difference.\n",
        "\n",
        "*\"We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7CXmJDh7Hwj",
        "colab_type": "text"
      },
      "source": [
        "#### Interpretation 2:\n",
        "\n",
        "Skip connections: direct gradient feedback from the output to each layer.\n",
        "\n",
        "\n",
        "2 layers no skip connection: $ NN(x) = G[F(x)] $\n",
        "\n",
        "$$ \\frac{dG}{dW}  =  G'[F(x)] \\times \\frac{dF}{dW} $$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The formula shows that the gradient of the first layer will get mutiplied by the derivative of each subsequent function in the networks. If the derivatives of the subsequent layers contain some noise (they are calculated with respect to the minibatch only) and there are many layers the real signal in the gradient might vanish, and it can become too 'noisy'.\n",
        "\n",
        "---\n",
        "\n",
        "2 layers with skip connection: $ NN(x) = G[F(x)] + F(x) $\n",
        "\n",
        "$$ \\frac{dG}{dW}  =   G'[F(x)] \\times \\frac{dF}{dW} +  \\frac{dF}{dW}  $$\n",
        "\n",
        "---\n",
        "\n",
        "the fi\n",
        "\n",
        "As you see skip connections introduce a direct path for the F function with w parameters to act on the output, and therefore a direct path appears in the gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B2-pXuK7Hwj",
        "colab_type": "text"
      },
      "source": [
        "### See with 3 layers \n",
        "\n",
        "3 layers no skip: $ NN(x) = H\\{G[F(x)]\\} $\n",
        "\n",
        "$$ \\frac{dH}{dW}  = H'\\{G[F(x)]\\} \\times G'[F(x)] \\times \\frac{dF}{dW} $$\n",
        "\n",
        "\n",
        "3 layers with  skip:  $ NN(x) = H\\{G[F(x)]+G[F(x)]\\} + G[F(x)] + F(x) $\n",
        "\n",
        "\n",
        "\n",
        "$$ \\frac{dH}{dW}  = H'\\{G[F(x)] + F(x)\\} \\times G'[F(x)] \\times \\frac{dF}{dW} \\\\\n",
        "+ H'\\{G[F(x)] + F(x)\\} \\times \\frac{dF}{dW} \\\\ +  G'[F(x)] \\times \\frac{dF}{dW} \\\\ + \\frac{dF}{dW} $$\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "With skip connections each individual path from the input to the output corresponds to a part of the sum in the gradient. With 4 paths we get the sum of 4 parts. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E06op4267Hwk",
        "colab_type": "text"
      },
      "source": [
        "#### The original Resnet block\n",
        "\n",
        "* bottleneck\n",
        "* \\+ batchnorm\n",
        "\n",
        "\n",
        "There are many-many variants now: with or without Bottleneck, place of relu and activation ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh-ohnZM7Hwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import BatchNormalization,Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrdL7wFu7Hwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Input(shape=(None,None,256))\n",
        "# 3x3 conv with 3 output channels (same as input channels)\n",
        "y = Conv2D(64, (1, 1), padding='same')(x)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "# 3x3 conv with 3 output channels (same as input channels)\n",
        "y = Conv2D(64, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "# 3x3 conv with 3 output channels (same as input channels)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "# this returns x + y.\n",
        "out = keras.layers.add([x, y])\n",
        "out = Activation('relu')(out)\n",
        "\n",
        "model = Model(inputs=x, outputs=out)\n",
        "SVG(model_to_dot(model,show_shapes=True, show_layer_names=False\n",
        "                ).create(prog='dot', format='svg',))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014RoXnR7Hwo",
        "colab_type": "text"
      },
      "source": [
        "## Batch normalization\n",
        "\n",
        "Original article: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "* \"The training is complicated by the fact that the inputs to each layer are affected by the parameters of all preceding layers so that small changes to the network parameters amplify as the network becomes deeper.\"\n",
        "\n",
        "* \"The change in the distributions of layers’ inputs presents a problem because the layers need to continuously adapt to the new distribution. When the input distribution to a learning system changes, it is said to experience covariate shift\"\n",
        "\n",
        "\n",
        "The normalization is a mean substraction and division by the std. ( And a learned scaling after the normalization). \n",
        "\n",
        "The values are calculated among each example of the same unit in the minibatch. For dense layers it practically means that minibatches should not be smaller than 16. For convolution layers, the aggregation is also performed along the spatial axis.\n",
        "\n",
        "During inference, the mean and std for the whole dataset is used, this is calculated during training with a running average.\n",
        "\n",
        "\n",
        "\n",
        "#### Practially:\n",
        "- Radically speeds up traninig\n",
        "- Eliminates the need for Droputs\n",
        "- Leads to better generalization\n",
        "\n",
        "\n",
        "#### More:\n",
        "- Batch renormalization: https://arxiv.org/abs/1702.03275\n",
        "- Self normalizing units, SeLU: https://arxiv.org/abs/1706.02515"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZATEMdB7Hwp",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "# And a few more:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgbhidQD7Hwp",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "## Inceptions (-Resnet)\n",
        "\n",
        "The continued development on inception (GoogleNet) models lead to one of the state of the art models for images classification. These are highly tuned, and very effective models developped by a research group in Google.\n",
        "\n",
        "\n",
        "Articles:\n",
        "* [Inception3](https://arxiv.org/abs/1512.00567)\n",
        "* [Inception4, Inception-Resnet](https://arxiv.org/abs/1602.07261)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ng4_lu7Hwp",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## ResNext\n",
        "\n",
        "\n",
        "- Group convolutions to further reduce parameters/computation eith similar repsentation power\n",
        "\n",
        "\n",
        "![resnext](http://www.deeplearningpatterns.com/lib/exe/fetch.php?media=wiki:resxnet.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP0RBYU97Hwq",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## DenseNet\n",
        "\n",
        "A lot more skip connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEEzNK_p7Hwq",
        "colab_type": "text"
      },
      "source": [
        "![densenet](https://data-sci.info/wp-content/uploads/2017/07/densenet-3.png)\n",
        "\n",
        "![densenet](https://d2mxuefqeaa7sj.cloudfront.net/s_8C67F3B1EB0F5ABE74BEB3BC706769E48F9A2C9CE88212D04817755A2125A582_1495365425569_image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mehM139N7Hwr",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "\n",
        "## SEnet\n",
        "\n",
        "- 2017 (last) ILSVRC winners\n",
        "\n",
        "![src](https://cdn-images-1.medium.com/max/2000/1*bmObF5Tibc58iE9iOu327w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVzGRfDW7Hwr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Summary:\n",
        "\n",
        "Lots of architectural improvements reducing single model errors 4-5 fold since 2012, for major benchmarks. It is also contiuously happening, you need to follow the news.\n",
        "\n",
        "- Batch normalization, always!\n",
        "- No need for the final dense layers, Dropouts not really important either.\n",
        "- Bottlenecks for better representation with fewer parameters.\n",
        "- Residual connections in order to train really deep models.\n",
        "- Inceptions are highly tuned models for general image classification.\n",
        "\n",
        "\n",
        "State of the art models generally need a lot of resources, but interestingly huge models (70-80M params) can be trained on relatively small datasets, like CIFAR10 too.\n",
        "\n",
        "Original implementations are usually published, or tested reimplementation appear after short time in one of the usual frameworks (Caffe, Torch, Tensorflow, Keras etc). But usually you can implement it yourself too!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvU0A90X7lbO",
        "colab_type": "text"
      },
      "source": [
        "## Bonus (not so good ) ResNext implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knju4aM07ecb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Train resnext of cifar10.\n",
        "The implementation follows the FAIR github repo:\n",
        "https://github.com/facebookresearch/ResNeXt\n",
        "which is slightly different than the arxiv report.\n",
        "Here I use the non-preactivation blocks.\n",
        "The specfic settings (lr,batch size ) can be changed \n",
        "to follow the original values (with enough GPUs, and time).\n",
        "Author: Dezso Ribli\n",
        "\"\"\"\n",
        "\n",
        "CARDINALITY = 16\n",
        "LR = 0.0125\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_DROP = [150,225]\n",
        "N_EPOCHS = 250\n",
        "AUG = True\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Activation\n",
        "from keras.layers import BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras import optimizers\n",
        "import math\n",
        "\n",
        "\n",
        "def resnext(inp, resxt_block, cardinality=4):\n",
        "    \"\"\"Return resnext.\"\"\"\n",
        "    # inital conv\n",
        "    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(5e-4))(inp)\n",
        "    x = Activation('relu')(BatchNormalization()(x))\n",
        "    # residual blocks\n",
        "    x = resxt_blocks(x, resxt_block, cardinality, 64, 256, 1)\n",
        "    x = resxt_blocks(x, resxt_block, cardinality, 128, 512, 2)\n",
        "    x = resxt_blocks(x, resxt_block, cardinality, 256, 1024, 2)\n",
        "    # classifier\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(10,activation='softmax')(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resxt_blocks(x, resxt_block, cardinality, n_ch1, n_ch2, init_stride, n_block=3):\n",
        "    \"\"\"Perform same size residual blocks.\"\"\"\n",
        "    x_shortcut = Conv2D(n_ch2, (1, 1), strides = init_stride, \n",
        "                        padding='same', kernel_regularizer=l2(5e-4))(x)\n",
        "    x_shortcut = BatchNormalization()(x_shortcut)\n",
        "    # first block\n",
        "    x = resxt_block(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride)\n",
        "    for i in range(n_block-1):  # the other residual blocks\n",
        "        x = resxt_block(x, x, cardinality, n_ch1, n_ch2)\n",
        "    return x\n",
        "         \n",
        "\n",
        "def resxt_block_a(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride=1):\n",
        "    \"\"\"Perform a residual block.\"\"\"\n",
        "    groups=[]\n",
        "    for i in range(cardinality):\n",
        "        y = Conv2D(n_ch1, (1, 1), strides=init_stride, \n",
        "                   kernel_regularizer=l2(5e-4), padding='same')(x)\n",
        "        y = Activation('relu')(BatchNormalization()(y))\n",
        "        y = Conv2D(n_ch1, (3, 3), padding='same', \n",
        "                   kernel_regularizer=l2(5e-4),)(y)\n",
        "        y = Activation('relu')(BatchNormalization()(y))\n",
        "        y = Conv2D(n_ch2, (1, 1), padding='same', \n",
        "                   kernel_regularizer=l2(5e-4),)(y)\n",
        "        y = BatchNormalization()(y)\n",
        "        groups.append(y)\n",
        "    x = keras.layers.add(groups)\n",
        "    x = keras.layers.add([x, x_shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x   \n",
        "\n",
        "\n",
        "def resxt_block_b(x, x_shortcut, cardinality, n_ch1, n_ch2, init_stride=1):\n",
        "    \"\"\"Perform a residual block.\"\"\"\n",
        "    groups=[]\n",
        "    for i in range(cardinality):\n",
        "        y = Conv2D(n_ch1, (1, 1), strides=init_stride, \n",
        "                   kernel_regularizer=l2(5e-4), padding='same')(x)\n",
        "        y = Activation('relu')(BatchNormalization()(y))\n",
        "        y = Conv2D(n_ch1, (3, 3), padding='same', \n",
        "                   kernel_regularizer=l2(5e-4),)(y)\n",
        "        y = Activation('relu')(BatchNormalization()(y))\n",
        "        groups.append(y)\n",
        "    x = keras.layers.concatenate(groups)\n",
        "    x = Conv2D(n_ch2, (1, 1), padding='same', \n",
        "               kernel_regularizer=l2(5e-4),)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = keras.layers.add([x, x_shortcut])\n",
        "    x = Activation('relu')(x)\n",
        "    return x   \n",
        "\n",
        "\n",
        "def norm(x):\n",
        "    \"\"\"Normalize images.\"\"\"\n",
        "    x = x.astype('float32')\n",
        "    x[...,0] = (x[...,0] - x[...,0].mean())/x[...,0].std()\n",
        "    x[...,1] = (x[...,1] - x[...,1].mean())/x[...,1].std()\n",
        "    x[...,2] = (x[...,2] - x[...,2].mean())/x[...,2].std()\n",
        "    return x\n",
        "\n",
        "\n",
        "def step_decay(epoch, base_lr=LR, drop=0.1, epochs_drops=EPOCHS_DROP):\n",
        "    \"\"\"Helper for step learning rate decay.\"\"\"\n",
        "    lrate = base_lr\n",
        "    for epoch_drop in epochs_drops:\n",
        "        lrate *= math.pow(drop,math.floor(epoch/epoch_drop))\n",
        "        return lrate\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # SGD\n",
        "    sgd = optimizers.SGD(lr=LR, decay=0, momentum=0.9, nesterov=True)\n",
        "    \n",
        "    # resnext\n",
        "    res = resnext(Input(shape=(32,32,3)), resxt_block_b, CARDINALITY)\n",
        "    res.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer=sgd, metrics=['accuracy'])\n",
        "    print res.summary()  # print summary\n",
        "     \n",
        "    # load data\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    x_train, x_test = map(norm, (x_train, x_test))  # normalize\n",
        "    \n",
        "    if AUG:\n",
        "        # train on generator with standard data augmentation\n",
        "        gen = ImageDataGenerator(width_shift_range=0.125,\n",
        "                                 height_shift_range=0.125,\n",
        "                                 horizontal_flip=True)\n",
        "        train_generator = gen.flow(x_train, y_train,\n",
        "                                   batch_size=BATCH_SIZE)\n",
        "        # train\n",
        "        res.fit_generator(train_generator, epochs=N_EPOCHS,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  callbacks=[LearningRateScheduler(step_decay)])\n",
        "    else:\n",
        "        # just train on data\n",
        "        res.fit(x_train, y_train, batch_size=BATCH_SIZE, \n",
        "                epochs=N_EPOCHS, validation_data=(x_test, y_test),\n",
        "                callbacks=[LearningRateScheduler(step_decay)])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}