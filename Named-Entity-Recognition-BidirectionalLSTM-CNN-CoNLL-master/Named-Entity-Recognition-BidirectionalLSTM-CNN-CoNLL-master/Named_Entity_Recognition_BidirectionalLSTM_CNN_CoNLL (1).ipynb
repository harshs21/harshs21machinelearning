{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwjgrpIbEXVX",
        "colab_type": "text"
      },
      "source": [
        "https://paperswithcode.com/paper/named-entity-recognition-with-bidirectional\n",
        "\n",
        "https://github.com/mxhofer/Named-Entity-Recognition-BidirectionalLSTM-CNN-CoNLL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NniI1OeEkrzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "862052a6-364c-4809-cfff-115847560d16"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def readfile(filename, *, encoding=\"UTF8\"):\n",
        "    '''\n",
        "    read file\n",
        "    return format :\n",
        "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
        "    '''\n",
        "    with open(filename, mode='rt', encoding=encoding) as f:\n",
        "        sentences = []\n",
        "        sentence = []\n",
        "        for line in f:\n",
        "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
        "                if len(sentence) > 0:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "                continue\n",
        "            splits = line.split(' ')\n",
        "            sentence.append([splits[0], splits[-1]])\n",
        "\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# define casing s.t. NN can use case information to learn patterns\n",
        "def getCasing(word, caseLookup):\n",
        "    casing = 'other'\n",
        "\n",
        "    numDigits = 0\n",
        "    for char in word:\n",
        "        if char.isdigit():\n",
        "            numDigits += 1\n",
        "\n",
        "    digitFraction = numDigits / float(len(word))\n",
        "\n",
        "    if word.isdigit():  # Is a digit\n",
        "        casing = 'numeric'\n",
        "    elif digitFraction > 0.5:\n",
        "        casing = 'mainly_numeric'\n",
        "    elif word.islower():  # All lower case\n",
        "        casing = 'allLower'\n",
        "    elif word.isupper():  # All upper case\n",
        "        casing = 'allUpper'\n",
        "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
        "        casing = 'initialUpper'\n",
        "    elif numDigits > 0:\n",
        "        casing = 'contains_digit'\n",
        "\n",
        "    return caseLookup[casing]\n",
        "\n",
        "\n",
        "# return batches ordered by words in sentence\n",
        "def createEqualBatches(data):\n",
        "    \n",
        "    \n",
        "    # num_words = []\n",
        "    # for i in data:\n",
        "    #     num_words.append(len(i[0]))\n",
        "    # num_words = set(num_words)\n",
        "    \n",
        "    n_batches = 100\n",
        "    batch_size = len(data) // n_batches\n",
        "    num_words = [batch_size*(i+1) for i in range(0, n_batches)]\n",
        "    \n",
        "    batches = []\n",
        "    batch_len = []\n",
        "    z = 0\n",
        "    start = 0\n",
        "    for end in num_words:\n",
        "        # print(\"start\", start)\n",
        "        for batch in data[start:end]:\n",
        "            # if len(batch[0]) == i:  # if sentence has i words\n",
        "            batches.append(batch)\n",
        "            z += 1\n",
        "        batch_len.append(z)\n",
        "        start = end\n",
        "\n",
        "    return batches, batch_len\n",
        "\n",
        "def createBatches(data):\n",
        "    l = []\n",
        "    for i in data:\n",
        "        l.append(len(i[0]))\n",
        "    l = set(l)\n",
        "    batches = []\n",
        "    batch_len = []\n",
        "    z = 0\n",
        "    for i in l:\n",
        "        for batch in data:\n",
        "            if len(batch[0]) == i:\n",
        "                batches.append(batch)\n",
        "                z += 1\n",
        "        batch_len.append(z)\n",
        "    return batches,batch_len\n",
        "\n",
        "\n",
        "# returns matrix with 1 entry = list of 4 elements:\n",
        "# word indices, case indices, character indices, label indices\n",
        "def createMatrices(sentences, word2Idx, label2Idx, case2Idx, char2Idx):\n",
        "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
        "    paddingIdx = word2Idx['PADDING_TOKEN']\n",
        "\n",
        "    dataset = []\n",
        "\n",
        "    wordCount = 0\n",
        "    unknownWordCount = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        wordIndices = []\n",
        "        caseIndices = []\n",
        "        charIndices = []\n",
        "        labelIndices = []\n",
        "\n",
        "        for word, char, label in sentence:\n",
        "            wordCount += 1\n",
        "            if word in word2Idx:\n",
        "                wordIdx = word2Idx[word]\n",
        "            elif word.lower() in word2Idx:\n",
        "                wordIdx = word2Idx[word.lower()]\n",
        "            else:\n",
        "                wordIdx = unknownIdx\n",
        "                unknownWordCount += 1\n",
        "            charIdx = []\n",
        "            for x in char:\n",
        "                charIdx.append(char2Idx[x])\n",
        "            # Get the label and map to int\n",
        "            wordIndices.append(wordIdx)\n",
        "            caseIndices.append(getCasing(word, case2Idx))\n",
        "            charIndices.append(charIdx)\n",
        "            labelIndices.append(label2Idx[label])\n",
        "\n",
        "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices])\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def iterate_minibatches(dataset, batch_len):\n",
        "    start = 0\n",
        "    for i in batch_len:\n",
        "        tokens = []\n",
        "        caseing = []\n",
        "        char = []\n",
        "        labels = []\n",
        "        data = dataset[start:i]\n",
        "        start = i\n",
        "        for dt in data:\n",
        "            t, c, ch, l = dt\n",
        "            l = np.expand_dims(l, -1)\n",
        "            tokens.append(t)\n",
        "            caseing.append(c)\n",
        "            char.append(ch)\n",
        "            labels.append(l)\n",
        "        \n",
        "        yield np.asarray(labels), np.asarray(tokens), np.asarray(caseing), np.asarray(char)        \n",
        "\n",
        "\n",
        "# returns data with character information in format\n",
        "# [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "def addCharInformation(Sentences):\n",
        "    for i, sentence in enumerate(Sentences):\n",
        "        for j, data in enumerate(sentence):\n",
        "            chars = [c for c in data[0]]\n",
        "            Sentences[i][j] = [data[0], chars, data[1]]\n",
        "    return Sentences\n",
        "\n",
        "\n",
        "# 0-pads all words\n",
        "def padding(Sentences):\n",
        "    maxlen = 52\n",
        "    for sentence in Sentences:\n",
        "        char = sentence[2]\n",
        "        for x in char:\n",
        "            maxlen = max(maxlen, len(x))\n",
        "    for i, sentence in enumerate(Sentences):\n",
        "        Sentences[i][2] = pad_sequences(Sentences[i][2], 52, padding='post')\n",
        "    return Sentences"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kK5TkNHwg24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Method to compute the accuracy. Call predict_labels to get the labels for the dataset\n",
        "def compute_f1(predictions, correct, idx2Label):\n",
        "    label_pred = []\n",
        "    for sentence in predictions:\n",
        "        label_pred.append([idx2Label[element] for element in sentence])\n",
        "\n",
        "    label_correct = []\n",
        "    for sentence in correct:\n",
        "        label_correct.append([idx2Label[element] for element in sentence])\n",
        "\n",
        "    # print(\"predictions \", len(label_pred))\n",
        "    # print(\"correct labels \", len(label_correct))\n",
        "\n",
        "    prec = compute_precision(label_pred, label_correct)\n",
        "    rec = compute_precision(label_correct, label_pred)\n",
        "\n",
        "    f1 = 0\n",
        "    if (rec + prec) > 0:\n",
        "        f1 = 2.0 * prec * rec / (prec + rec);\n",
        "\n",
        "    return prec, rec, f1\n",
        "\n",
        "\n",
        "def compute_precision(guessed_sentences, correct_sentences):\n",
        "    assert (len(guessed_sentences) == len(correct_sentences))\n",
        "    correctCount = 0\n",
        "    count = 0\n",
        "\n",
        "    for sentenceIdx in range(len(guessed_sentences)):\n",
        "        guessed = guessed_sentences[sentenceIdx]\n",
        "        correct = correct_sentences[sentenceIdx]\n",
        "        assert (len(guessed) == len(correct))\n",
        "        idx = 0\n",
        "        while idx < len(guessed):\n",
        "            if guessed[idx][0] == 'B':  # a new chunk starts\n",
        "                count += 1\n",
        "\n",
        "                if guessed[idx] == correct[idx]:  # first prediction correct\n",
        "                    idx += 1\n",
        "                    correctlyFound = True\n",
        "\n",
        "                    while idx < len(guessed) and guessed[idx][0] == 'I':  # scan entire chunk\n",
        "                        if guessed[idx] != correct[idx]:\n",
        "                            correctlyFound = False \n",
        "\n",
        "                        idx += 1\n",
        "\n",
        "                    if idx < len(guessed):\n",
        "                        if correct[idx][0] == 'I':  # chunk in correct was longer\n",
        "                            correctlyFound = False\n",
        "\n",
        "                    if correctlyFound:\n",
        "                        correctCount += 1\n",
        "                else:\n",
        "                    idx += 1\n",
        "            else:\n",
        "                idx += 1\n",
        "\n",
        "    precision = 0\n",
        "    if count > 0:\n",
        "        precision = float(correctCount) / count\n",
        "\n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fU7J1H9wrC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate\n",
        "\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.optimizers import SGD, Nadam\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oveKGg1wzz5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "415cc0fb-5d74-4be4-bf46-d3ddd3fd8baa"
      },
      "source": [
        "\"\"\"Initialise class\"\"\"\n",
        "\n",
        "class CNN_BLSTM(object):\n",
        "    \n",
        "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
        "        \n",
        "        self.epochs = EPOCHS\n",
        "        self.dropout = DROPOUT\n",
        "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
        "        self.lstm_state_size = LSTM_STATE_SIZE\n",
        "        self.conv_size = CONV_SIZE\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.optimizer = OPTIMIZER\n",
        "        \n",
        "    def loadData(self):\n",
        "        \"\"\"Load data and add character information\"\"\"\n",
        "        self.trainSentences = readfile(\"train.txt\")\n",
        "        self.devSentences = readfile(\"dev.txt\")\n",
        "        self.testSentences = readfile(\"test.txt\")\n",
        "\n",
        "    def addCharInfo(self):\n",
        "        # format: [['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
        "        self.trainSentences = addCharInformation(self.trainSentences)\n",
        "        self.devSentences = addCharInformation(self.devSentences)\n",
        "        self.testSentences = addCharInformation(self.testSentences)\n",
        "\n",
        "    def embed(self):\n",
        "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
        "\n",
        "        labelSet = set()\n",
        "        words = {}\n",
        "\n",
        "        # unique words and labels in data  \n",
        "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
        "            for sentence in dataset:\n",
        "                for token, char, label in sentence:\n",
        "                    # token ... token, char ... list of chars, label ... BIO labels   \n",
        "                    labelSet.add(label)\n",
        "                    words[token.lower()] = True\n",
        "\n",
        "        # mapping for labels\n",
        "        self.label2Idx = {}\n",
        "        for label in labelSet:\n",
        "            self.label2Idx[label] = len(self.label2Idx)\n",
        "\n",
        "        # mapping for token cases\n",
        "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
        "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
        "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
        "\n",
        "        # read GLoVE word embeddings\n",
        "        word2Idx = {}\n",
        "        self.wordEmbeddings = []\n",
        "\n",
        "        fEmbeddings = open(\"glove.6B.50d.txt\", encoding=\"utf-8\")\n",
        "\n",
        "        # loop through each word in embeddings\n",
        "        for line in fEmbeddings:\n",
        "            split = line.strip().split(\" \")\n",
        "            word = split[0]  # embedding word entry\n",
        "\n",
        "            if len(word2Idx) == 0:  # add padding+unknown\n",
        "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
        "                self.wordEmbeddings.append(vector)\n",
        "\n",
        "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
        "                self.wordEmbeddings.append(vector)\n",
        "\n",
        "            if split[0].lower() in words:\n",
        "                vector = np.array([float(num) for num in split[1:]])\n",
        "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
        "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
        "\n",
        "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
        "\n",
        "        # dictionary of all possible characters\n",
        "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
        "        for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n",
        "            self.char2Idx[c] = len(self.char2Idx)\n",
        "\n",
        "        # format: [[wordindices], [caseindices], [padded word indices], [label indices]]\n",
        "        self.train_set = padding(createMatrices(self.trainSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.dev_set = padding(createMatrices(self.devSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
        "        self.test_set = padding(createMatrices(self.testSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
        "\n",
        "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}\n",
        "        \n",
        "    def createBatches(self):\n",
        "        \"\"\"Create batches\"\"\"\n",
        "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
        "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
        "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
        "        \n",
        "    def tag_dataset(self, dataset, model):\n",
        "        \"\"\"Tag data with numerical values\"\"\"\n",
        "        correctLabels = []\n",
        "        predLabels = []\n",
        "        for i, data in enumerate(dataset):\n",
        "            tokens, casing, char, labels = data\n",
        "            tokens = np.asarray([tokens])\n",
        "            casing = np.asarray([casing])\n",
        "            char = np.asarray([char])\n",
        "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
        "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
        "            correctLabels.append(labels)\n",
        "            predLabels.append(pred)\n",
        "        return predLabels, correctLabels\n",
        "    \n",
        "    def buildModel(self):\n",
        "        \"\"\"Model layers\"\"\"\n",
        "\n",
        "        # character input\n",
        "        character_input = Input(shape=(None, 52,), name=\"Character_input\")\n",
        "        embed_char_out = TimeDistributed(\n",
        "            Embedding(len(self.char2Idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
        "            character_input)\n",
        "\n",
        "        dropout = Dropout(self.dropout)(embed_char_out)\n",
        "\n",
        "        # CNN\n",
        "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
        "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n",
        "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "        char = Dropout(self.dropout)(char)\n",
        "\n",
        "        # word-level input\n",
        "        words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
        "        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n",
        "                          trainable=False)(words_input)\n",
        "\n",
        "        # case-info input\n",
        "        casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], weights=[self.caseEmbeddings],\n",
        "                           trainable=False)(casing_input)\n",
        "\n",
        "        # concat & BLSTM\n",
        "        output = concatenate([words, casing, char])\n",
        "        output = Bidirectional(LSTM(self.lstm_state_size, \n",
        "                                    return_sequences=True, \n",
        "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
        "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
        "                                   ), name=\"BLSTM\")(output)\n",
        "        output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(output)\n",
        "\n",
        "        # set up model\n",
        "        self.model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
        "        \n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
        "        \n",
        "        self.init_weights = self.model.get_weights()\n",
        "        \n",
        "        plot_model(self.model, to_file='model.png')\n",
        "        \n",
        "        print(\"Model built. Saved model.png\\n\")\n",
        "        \n",
        "    def train(self):\n",
        "        \"\"\"Default training\"\"\"\n",
        "\n",
        "        self.f1_test_history = []\n",
        "        self.f1_dev_history = []\n",
        "\n",
        "        for epoch in range(self.epochs):    \n",
        "            print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
        "            for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
        "                labels, tokens, casing,char = batch       \n",
        "                self.model.train_on_batch([tokens, casing,char], labels)\n",
        "\n",
        "            # compute F1 scores\n",
        "            predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
        "            pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_test_history.append(f1_test)\n",
        "            print(\"f1 test \", round(f1_test, 4))\n",
        "\n",
        "            predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
        "            pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
        "            self.f1_dev_history.append(f1_dev)\n",
        "            print(\"f1 dev \", round(f1_dev, 4), \"\\n\")\n",
        "            \n",
        "        print(\"Final F1 test score: \", f1_test)\n",
        "            \n",
        "        print(\"Training finished.\")\n",
        "            \n",
        "        # save model\n",
        "        self.modelName = \"{}_{}_{}_{}_{}_{}_{}\".format(self.epochs, \n",
        "                                                        self.dropout, \n",
        "                                                        self.dropout_recurrent, \n",
        "                                                        self.lstm_state_size,\n",
        "                                                        self.conv_size,\n",
        "                                                        self.learning_rate,\n",
        "                                                        self.optimizer.__class__.__name__\n",
        "                                                       )\n",
        "        \n",
        "        modelName = self.modelName + \".h5\"\n",
        "        self.model.save(modelName)\n",
        "        print(\"Model weights saved.\")\n",
        "        \n",
        "        self.model.set_weights(self.init_weights)  # clear model\n",
        "        print(\"Model weights cleared.\")\n",
        "\n",
        "    def writeToFile(self):\n",
        "        \"\"\"Write output to file\"\"\"\n",
        "\n",
        "        # .txt file format\n",
        "        # [epoch  ]\n",
        "        # [f1_test]\n",
        "        # [f1_dev ]\n",
        "        \n",
        "        output = np.matrix([[int(i) for i in range(self.epochs)], self.f1_test_history, self.f1_dev_history])\n",
        "\n",
        "        fileName = self.modelName + \".txt\"\n",
        "        with open(fileName,'wb') as f:\n",
        "            for line in output:\n",
        "                np.savetxt(f, line, fmt='%.5f')\n",
        "                \n",
        "        print(\"Model performance written to file.\")\n",
        "\n",
        "    print(\"Class initialised.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class initialised.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aere-k5033HM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Set parameters\"\"\"\n",
        "\n",
        "EPOCHS = 30               # paper: 80\n",
        "DROPOUT = 0.5             # paper: 0.68\n",
        "DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n",
        "LSTM_STATE_SIZE = 200     # paper: 275\n",
        "CONV_SIZE = 3             # paper: 3\n",
        "LEARNING_RATE = 0.0105    # paper 0.0105\n",
        "OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2H9fHk24BkS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36af80ea-82fc-4f30-dfa6-712ccb153405"
      },
      "source": [
        "\"\"\"Construct and run model\"\"\"\n",
        "\n",
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
        "cnn_blstm.loadData()\n",
        "cnn_blstm.addCharInfo()\n",
        "cnn_blstm.embed()\n",
        "cnn_blstm.createBatches()\n",
        "cnn_blstm.buildModel()\n",
        "cnn_blstm.train()\n",
        "cnn_blstm.writeToFile()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0528 19:25:19.596649 140034741917568 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model built. Saved model.png\n",
            "\n",
            "Epoch 0/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0528 19:25:21.232932 140034741917568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0528 19:25:22.965499 140034741917568 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "f1 test  0.44\n",
            "f1 dev  0.4798 \n",
            "\n",
            "Epoch 1/30\n",
            "f1 test  0.5869\n",
            "f1 dev  0.6349 \n",
            "\n",
            "Epoch 2/30\n",
            "f1 test  0.6718\n",
            "f1 dev  0.6874 \n",
            "\n",
            "Epoch 3/30\n",
            "f1 test  0.7476\n",
            "f1 dev  0.7624 \n",
            "\n",
            "Epoch 4/30\n",
            "f1 test  0.7295\n",
            "f1 dev  0.7338 \n",
            "\n",
            "Epoch 5/30\n",
            "f1 test  0.7651\n",
            "f1 dev  0.7735 \n",
            "\n",
            "Epoch 6/30\n",
            "f1 test  0.7917\n",
            "f1 dev  0.8118 \n",
            "\n",
            "Epoch 7/30\n",
            "f1 test  0.7855\n",
            "f1 dev  0.7928 \n",
            "\n",
            "Epoch 8/30\n",
            "f1 test  0.7899\n",
            "f1 dev  0.8032 \n",
            "\n",
            "Epoch 9/30\n",
            "f1 test  0.8177\n",
            "f1 dev  0.8399 \n",
            "\n",
            "Epoch 10/30\n",
            "f1 test  0.8154\n",
            "f1 dev  0.83 \n",
            "\n",
            "Epoch 11/30\n",
            "f1 test  0.8264\n",
            "f1 dev  0.8436 \n",
            "\n",
            "Epoch 12/30\n",
            "f1 test  0.8301\n",
            "f1 dev  0.8604 \n",
            "\n",
            "Epoch 13/30\n",
            "f1 test  0.8301\n",
            "f1 dev  0.8451 \n",
            "\n",
            "Epoch 14/30\n",
            "f1 test  0.8417\n",
            "f1 dev  0.8566 \n",
            "\n",
            "Epoch 15/30\n",
            "f1 test  0.8375\n",
            "f1 dev  0.8598 \n",
            "\n",
            "Epoch 16/30\n",
            "f1 test  0.8319\n",
            "f1 dev  0.8606 \n",
            "\n",
            "Epoch 17/30\n",
            "f1 test  0.8439\n",
            "f1 dev  0.8735 \n",
            "\n",
            "Epoch 18/30\n",
            "f1 test  0.8511\n",
            "f1 dev  0.8818 \n",
            "\n",
            "Epoch 19/30\n",
            "f1 test  0.8447\n",
            "f1 dev  0.8705 \n",
            "\n",
            "Epoch 20/30\n",
            "f1 test  0.8494\n",
            "f1 dev  0.8679 \n",
            "\n",
            "Epoch 21/30\n",
            "f1 test  0.8489\n",
            "f1 dev  0.8781 \n",
            "\n",
            "Epoch 22/30\n",
            "f1 test  0.8477\n",
            "f1 dev  0.8669 \n",
            "\n",
            "Epoch 23/30\n",
            "f1 test  0.8542\n",
            "f1 dev  0.8844 \n",
            "\n",
            "Epoch 24/30\n",
            "f1 test  0.8462\n",
            "f1 dev  0.8734 \n",
            "\n",
            "Epoch 25/30\n",
            "f1 test  0.8609\n",
            "f1 dev  0.883 \n",
            "\n",
            "Epoch 26/30\n",
            "f1 test  0.8605\n",
            "f1 dev  0.8852 \n",
            "\n",
            "Epoch 27/30\n",
            "f1 test  0.8657\n",
            "f1 dev  0.8934 \n",
            "\n",
            "Epoch 28/30\n",
            "f1 test  0.8657\n",
            "f1 dev  0.8983 \n",
            "\n",
            "Epoch 29/30\n",
            "f1 test  0.8701\n",
            "f1 dev  0.8982 \n",
            "\n",
            "Final F1 test score:  0.8700634696755996\n",
            "Training finished.\n",
            "Model weights saved.\n",
            "Model weights cleared.\n",
            "Model performance written to file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6beFGK04OBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "65c09f5e-22a2-4f75-8a1f-4531a9d34363"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
        "plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjIoSeWxAZZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "44b50691-e25d-474e-e280-a7ffb59ce9f7"
      },
      "source": [
        "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
        "cnn_blstm.loadData()\n",
        "\n",
        "category_count = {\"B-ORG\\n\": 0, \"I-ORG\\n\":0, \"B-MISC\\n\": 0, \"I-MISC\\n\":0, \"B-LOC\\n\": 0, \"I-LOC\\n\": 0, \"B-PER\\n\": 0, \"I-PER\\n\": 0, \"O\\n\": 0}\n",
        "total_count = 0\n",
        "\n",
        "for sentence in cnn_blstm.trainSentences:\n",
        "    for word in sentence:\n",
        "        if word[1] in category_count.keys():\n",
        "            category_count[word[1]] += 1\n",
        "            total_count += 1\n",
        "\n",
        "for category, count in category_count.items():\n",
        "    print(\"{}: {}%\".format(category.replace(\"\\n\", \"\"), round((count/total_count)*100, 2)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B-ORG: 3.1%\n",
            "I-ORG: 1.82%\n",
            "B-MISC: 1.69%\n",
            "I-MISC: 0.57%\n",
            "B-LOC: 3.51%\n",
            "I-LOC: 0.57%\n",
            "B-PER: 3.24%\n",
            "I-PER: 2.22%\n",
            "O: 83.28%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsMWSMRcAl0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}