{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "NN with batch GD no optimizer.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MwP_aJZxXo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uBEOGKNxXpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet :\n",
        "\n",
        "    def __init__(self, layers=[200, 200, 10], learning_rate = 0.001, activation=['elu','elu','softmax'],\n",
        "                 epochs=100, elu_alpha=1.2, batch_size=250, l2_lambda = 1e-4, epsilon=1e-8,\n",
        "                 beta1=0.9, beta2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.num_layers = len(layers)\n",
        "        self.layers = layers\n",
        "        self.activation = activation\n",
        "        self.elu_alpha = elu_alpha\n",
        "        self.activate = {\n",
        "            'elu': self.elu_activation,\n",
        "            'softmax': self.softmax_activation\n",
        "        }\n",
        "        self.weights = []\n",
        "        self.bias = []\n",
        "        self.batch_size = batch_size\n",
        "        self.differentiate = {\n",
        "            'elu': self.d_elu_activation,\n",
        "            'softmax': self.d_softmax_activation\n",
        "        }\n",
        "        self.optimizer_cache = {}\n",
        "        self.l2_lambda = l2_lambda\n",
        "        self.epsilon = epsilon\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "\n",
        "    def forward_pass(self, train, save_cache=False):\n",
        "        cache = {\n",
        "            'scores': [],\n",
        "            'inputs': []\n",
        "        }\n",
        "        for i,n in enumerate(self.layers):\n",
        "            if i == 0:\n",
        "                Z = np.dot(train,self.weights[i]) + self.bias[i]\n",
        "            else:\n",
        "                Z = np.dot(A,self.weights[i]) + self.bias[i]\n",
        "            if save_cache:\n",
        "                cache['scores'].append(Z)\n",
        "                if i!=0:\n",
        "                    cache['inputs'].append(A)\n",
        "            A = self.activate[self.activation[i]](Z)\n",
        "        return (A, cache) if save_cache else (A, None)\n",
        "\n",
        "    def backpropogate_update(self, X_train, Y_train, prediction, cache, iter):\n",
        "        batch_size = X_train.shape[0]\n",
        "        d_output = self.d_categorical_cross_entropy_loss(Y_train,prediction)\n",
        "\n",
        "\n",
        "        for layer in range(len(self.layers)-1,0,-1):\n",
        "            d_score = d_output*self.differentiate[self.activation[layer]](cache['scores'][layer])\n",
        "            if layer==0:\n",
        "                d_weights = np.dot(d_score, X_train.T)/batch_size\n",
        "            else:\n",
        "                d_weights = np.dot(cache['inputs'][layer-1].T, d_score)/batch_size\n",
        "            d_bias = np.sum(d_score, axis=0, keepdims=True)\n",
        "            d_output = np.dot(d_score,self.weights[layer].T)\n",
        "\n",
        "\n",
        "            self.weights[layer] -= 0.001 * (self.l2_lambda*self.weights[layer])# l2_regularization\n",
        "            self.bias[layer] -= 0.001 * (self.l2_lambda*self.bias[layer])  # l2_regularization\n",
        "\n",
        "    def softmax_activation(self, Z):\n",
        "        Z_dash = Z - Z.max()  # for numerical stability\n",
        "        e = np.exp(Z_dash)\n",
        "        return e / (np.sum(e, axis=1, keepdims=True))\n",
        "\n",
        "    def d_softmax_activation(self, y):\n",
        "        return y * (1 - y)\n",
        "\n",
        "    def elu_activation(self, Z):\n",
        "        return np.where(Z >= 0, Z, self.elu_alpha*(np.exp(Z) - 1))\n",
        "\n",
        "    def d_elu_activation(self, Z):\n",
        "        return (Z >= 0).astype('float32') + (Z < 0).astype('float32') * (self.elu_activation(Z) + self.elu_alpha)\n",
        "\n",
        "    def categorical_cross_entropy_loss(self, actual, prediction):\n",
        "        prediction /= np.sum(prediction, axis=-1, keepdims=True)\n",
        "        prediction = np.clip(prediction, 10e-8, 1. - 10e-8)  # for numerical stability\n",
        "        return -np.sum(actual * np.log(prediction))\n",
        "\n",
        "    def d_categorical_cross_entropy_loss(self, actual, prediction):\n",
        "        return actual - prediction\n",
        "\n",
        "    def init_weights(self,M):\n",
        "        # using He normal initialization\n",
        "        weights = []\n",
        "        bias = []\n",
        "        for n,m in enumerate(self.layers):\n",
        "            if n==0:\n",
        "                weights.append(np.random.normal(0, np.sqrt(2/M),size=[M,m]))\n",
        "            else:\n",
        "                weights.append(np.random.normal(0,np.sqrt(2/self.layers[n-1]),size=[self.layers[n-1],m]))\n",
        "            bias.append(np.random.uniform(-0.2,0.2,size=[1,m]))\n",
        "        return weights, bias\n",
        "\n",
        "    def get_batch(self, X_train, Y_train):\n",
        "        n_batches = X_train.shape[0]//self.batch_size\n",
        "        if n_batches == 0:\n",
        "            yield X_train, Y_train\n",
        "        for i in range(n_batches):\n",
        "            if i==n_batches-1:\n",
        "                yield X_train[i*self.batch_size:, :], Y_train[i*self.batch_size:, :]\n",
        "            else:\n",
        "                yield X_train[i*self.batch_size:(i+1)*self.batch_size, :], Y_train[i*self.batch_size:(i+1)*self.batch_size, :]\n",
        "\n",
        "    def train(self, X_train, Y_train):\n",
        "        (N, M) = X_train.shape\n",
        "        self.weights, self.bias = self.init_weights(M)\n",
        "\n",
        "        iter = self.batch_size\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            shuffle_indices = np.random.permutation(X_train.shape[0])\n",
        "            X_train_shuffled = X_train[shuffle_indices]\n",
        "            Y_train_shuffled = Y_train[shuffle_indices]\n",
        "\n",
        "\n",
        "            for X_batch, Y_batch in self.get_batch(X_train_shuffled, Y_train_shuffled):\n",
        "                prediction, cache = self.forward_pass(X_batch, save_cache=True)\n",
        "\n",
        "                self.backpropogate_update(X_batch, Y_batch, prediction, cache, iter)\n",
        "\n",
        "                iter += self.batch_size\n",
        "\n",
        "            print(\"epoch {}: Training accuracy = {}\".format(epoch+1, accuracy(self.predict(X_train), Y_train)))\n",
        "\n",
        "    def predict(self,X):\n",
        "        n_batches = X.shape[0] // self.batch_size\n",
        "        output_size = self.layers[len(self.layers)-1]\n",
        "        if n_batches == 0:\n",
        "            predictions,cache = self.forward_pass(X,save_cache=False)\n",
        "        else:\n",
        "            predictions = np.zeros([X.shape[0],output_size])\n",
        "            for i in range(n_batches):\n",
        "                if i==n_batches-1:\n",
        "                    predictions[i*self.batch_size:], cache = self.forward_pass(X[i*self.batch_size:])\n",
        "                else:\n",
        "                    predictions[i * self.batch_size:(i+1)*self.batch_size], \\\n",
        "                    cache = self.forward_pass(X[i * self.batch_size: (i+1)*self.batch_size])\n",
        "        return predictions\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGU7xapVxXpQ",
        "colab_type": "code",
        "outputId": "294aa2d9-5dc0-4437-a055-3f54511b7a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('ex2data2.csv', sep = ',')\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "      <th>C</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.092742</td>\n",
              "      <td>0.68494</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.213710</td>\n",
              "      <td>0.69225</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.375000</td>\n",
              "      <td>0.50219</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.513250</td>\n",
              "      <td>0.46564</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.524770</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          A        B  C\n",
              "0 -0.092742  0.68494  1\n",
              "1 -0.213710  0.69225  1\n",
              "2 -0.375000  0.50219  1\n",
              "3 -0.513250  0.46564  1\n",
              "4 -0.524770  0.20980  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXcgPk_qxXpb",
        "colab_type": "code",
        "outputId": "2f66e846-77d4-4139-8969-fffa6dbeebdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train = df.iloc[:,:-1]\n",
        "Y_train = df.iloc[:, -1]\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117, 2)\n",
            "(117, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MBPr0x0xXpk",
        "colab_type": "code",
        "outputId": "a303491e-7874-4bfa-aa32-c682638667f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.092742  0.68494 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPGNGbIxxXp3",
        "colab_type": "code",
        "outputId": "e5fee15e-145d-45fc-c645-f802cdc09b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Split the data into test and train sets\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train, Y_train = shuffle(X_train, Y_train)\n",
        "\n",
        "X_test = X_train[85:,:]\n",
        "Y_test = Y_train[85:,:]\n",
        "\n",
        "X_train_ = X_train[:250,:]\n",
        "Y_train_ = Y_train[:250,:]\n",
        "\n",
        "print(X_train_.shape)\n",
        "print(Y_train_.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(117, 2)\n",
            "(117, 1)\n",
            "(32, 2)\n",
            "(32, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMTzdzNCxXqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(actual, prediction):\n",
        "    return np.mean(np.argmax(actual,axis=1)==np.argmax(prediction,axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R4vbEIBxXqK",
        "colab_type": "code",
        "outputId": "a47e1bff-8d04-449c-d077-bac206df871c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def main():\n",
        "  \n",
        "    print(\"training data shape: {}\".format(X_train.shape))\n",
        "    print(\"training labels shape: {}\".format(Y_train.shape))\n",
        "\n",
        "    nn = NeuralNet()\n",
        "    nn.train(X_train, Y_train)\n",
        "    test_pred = nn.predict(X_test)\n",
        "\n",
        "    print(\"Final Testing Accuracy = {}\".format(accuracy(Y_test, test_pred)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data shape: (117, 2)\n",
            "training labels shape: (117, 1)\n",
            "epoch 1: Training accuracy = 0.3247863247863248\n",
            "epoch 2: Training accuracy = 0.3247863247863248\n",
            "epoch 3: Training accuracy = 0.3247863247863248\n",
            "epoch 4: Training accuracy = 0.3247863247863248\n",
            "epoch 5: Training accuracy = 0.3247863247863248\n",
            "epoch 6: Training accuracy = 0.3247863247863248\n",
            "epoch 7: Training accuracy = 0.3247863247863248\n",
            "epoch 8: Training accuracy = 0.3247863247863248\n",
            "epoch 9: Training accuracy = 0.3247863247863248\n",
            "epoch 10: Training accuracy = 0.3247863247863248\n",
            "epoch 11: Training accuracy = 0.3247863247863248\n",
            "epoch 12: Training accuracy = 0.3247863247863248\n",
            "epoch 13: Training accuracy = 0.3247863247863248\n",
            "epoch 14: Training accuracy = 0.3247863247863248\n",
            "epoch 15: Training accuracy = 0.3247863247863248\n",
            "epoch 16: Training accuracy = 0.3247863247863248\n",
            "epoch 17: Training accuracy = 0.3247863247863248\n",
            "epoch 18: Training accuracy = 0.3247863247863248\n",
            "epoch 19: Training accuracy = 0.3247863247863248\n",
            "epoch 20: Training accuracy = 0.3247863247863248\n",
            "epoch 21: Training accuracy = 0.3247863247863248\n",
            "epoch 22: Training accuracy = 0.3247863247863248\n",
            "epoch 23: Training accuracy = 0.3247863247863248\n",
            "epoch 24: Training accuracy = 0.3247863247863248\n",
            "epoch 25: Training accuracy = 0.3247863247863248\n",
            "epoch 26: Training accuracy = 0.3247863247863248\n",
            "epoch 27: Training accuracy = 0.3247863247863248\n",
            "epoch 28: Training accuracy = 0.3247863247863248\n",
            "epoch 29: Training accuracy = 0.3247863247863248\n",
            "epoch 30: Training accuracy = 0.3247863247863248\n",
            "epoch 31: Training accuracy = 0.3247863247863248\n",
            "epoch 32: Training accuracy = 0.3247863247863248\n",
            "epoch 33: Training accuracy = 0.3247863247863248\n",
            "epoch 34: Training accuracy = 0.3247863247863248\n",
            "epoch 35: Training accuracy = 0.3247863247863248\n",
            "epoch 36: Training accuracy = 0.3247863247863248\n",
            "epoch 37: Training accuracy = 0.3247863247863248\n",
            "epoch 38: Training accuracy = 0.3247863247863248\n",
            "epoch 39: Training accuracy = 0.3247863247863248\n",
            "epoch 40: Training accuracy = 0.3247863247863248\n",
            "epoch 41: Training accuracy = 0.3247863247863248\n",
            "epoch 42: Training accuracy = 0.3247863247863248\n",
            "epoch 43: Training accuracy = 0.3247863247863248\n",
            "epoch 44: Training accuracy = 0.3247863247863248\n",
            "epoch 45: Training accuracy = 0.3247863247863248\n",
            "epoch 46: Training accuracy = 0.3247863247863248\n",
            "epoch 47: Training accuracy = 0.3247863247863248\n",
            "epoch 48: Training accuracy = 0.3247863247863248\n",
            "epoch 49: Training accuracy = 0.3247863247863248\n",
            "epoch 50: Training accuracy = 0.3247863247863248\n",
            "epoch 51: Training accuracy = 0.3247863247863248\n",
            "epoch 52: Training accuracy = 0.3247863247863248\n",
            "epoch 53: Training accuracy = 0.3247863247863248\n",
            "epoch 54: Training accuracy = 0.3247863247863248\n",
            "epoch 55: Training accuracy = 0.3247863247863248\n",
            "epoch 56: Training accuracy = 0.3247863247863248\n",
            "epoch 57: Training accuracy = 0.3247863247863248\n",
            "epoch 58: Training accuracy = 0.3247863247863248\n",
            "epoch 59: Training accuracy = 0.3247863247863248\n",
            "epoch 60: Training accuracy = 0.3247863247863248\n",
            "epoch 61: Training accuracy = 0.3247863247863248\n",
            "epoch 62: Training accuracy = 0.3247863247863248\n",
            "epoch 63: Training accuracy = 0.3247863247863248\n",
            "epoch 64: Training accuracy = 0.3247863247863248\n",
            "epoch 65: Training accuracy = 0.3247863247863248\n",
            "epoch 66: Training accuracy = 0.3247863247863248\n",
            "epoch 67: Training accuracy = 0.3247863247863248\n",
            "epoch 68: Training accuracy = 0.3247863247863248\n",
            "epoch 69: Training accuracy = 0.3247863247863248\n",
            "epoch 70: Training accuracy = 0.3247863247863248\n",
            "epoch 71: Training accuracy = 0.3247863247863248\n",
            "epoch 72: Training accuracy = 0.3247863247863248\n",
            "epoch 73: Training accuracy = 0.3247863247863248\n",
            "epoch 74: Training accuracy = 0.3247863247863248\n",
            "epoch 75: Training accuracy = 0.3247863247863248\n",
            "epoch 76: Training accuracy = 0.3247863247863248\n",
            "epoch 77: Training accuracy = 0.3247863247863248\n",
            "epoch 78: Training accuracy = 0.3247863247863248\n",
            "epoch 79: Training accuracy = 0.3247863247863248\n",
            "epoch 80: Training accuracy = 0.3247863247863248\n",
            "epoch 81: Training accuracy = 0.3247863247863248\n",
            "epoch 82: Training accuracy = 0.3247863247863248\n",
            "epoch 83: Training accuracy = 0.3247863247863248\n",
            "epoch 84: Training accuracy = 0.3247863247863248\n",
            "epoch 85: Training accuracy = 0.3247863247863248\n",
            "epoch 86: Training accuracy = 0.3247863247863248\n",
            "epoch 87: Training accuracy = 0.3247863247863248\n",
            "epoch 88: Training accuracy = 0.3247863247863248\n",
            "epoch 89: Training accuracy = 0.3247863247863248\n",
            "epoch 90: Training accuracy = 0.3247863247863248\n",
            "epoch 91: Training accuracy = 0.3247863247863248\n",
            "epoch 92: Training accuracy = 0.3247863247863248\n",
            "epoch 93: Training accuracy = 0.3247863247863248\n",
            "epoch 94: Training accuracy = 0.3247863247863248\n",
            "epoch 95: Training accuracy = 0.3247863247863248\n",
            "epoch 96: Training accuracy = 0.3247863247863248\n",
            "epoch 97: Training accuracy = 0.3247863247863248\n",
            "epoch 98: Training accuracy = 0.3247863247863248\n",
            "epoch 99: Training accuracy = 0.3247863247863248\n",
            "epoch 100: Training accuracy = 0.3247863247863248\n",
            "Final Testing Accuracy = 0.34375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A3OhDiWxXqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}