{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFAgarap Linear SVM Github.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNjWnwCrNg5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "__version__ = \"0.1.5\"\n",
        "__author__ = \"Abien Fred Agarap\"\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "class SVM:\n",
        "    \"\"\"Implementation of L2-Support Vector Machine using TensorFlow\"\"\"\n",
        "\n",
        "    def __init__(self, alpha, batch_size, svm_c, num_classes, num_features):\n",
        "        \"\"\"Initialize the SVM class\n",
        "        Parameter\n",
        "        ---------\n",
        "        alpha : float\n",
        "          The learning rate for the SVM model.\n",
        "        batch_size : int\n",
        "          Number of batches to use for training and testing.\n",
        "        svm_c : float\n",
        "          The SVM penalty parameter.\n",
        "        num_classes : int\n",
        "          Number of classes in a dataset.\n",
        "        num_features : int\n",
        "          Number of features in a dataset.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.batch_size = batch_size\n",
        "        self.svm_c = svm_c\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = num_features\n",
        "\n",
        "        def __graph__():\n",
        "            \"\"\"Building the inference graph\"\"\"\n",
        "\n",
        "            with tf.name_scope(\"input\"):\n",
        "                # [BATCH_SIZE, NUM_FEATURES]\n",
        "                x_input = tf.placeholder(\n",
        "                    dtype=tf.float32, shape=[None, self.num_features], name=\"x_input\"\n",
        "                )\n",
        "\n",
        "                # [BATCH_SIZE]\n",
        "                y_input = tf.placeholder(dtype=tf.uint8, shape=[None], name=\"y_input\")\n",
        "\n",
        "                # [BATCH_SIZE, NUM_CLASSES]\n",
        "                y_onehot = tf.one_hot(\n",
        "                    indices=y_input,\n",
        "                    depth=self.num_classes,\n",
        "                    on_value=1,\n",
        "                    off_value=-1,\n",
        "                    name=\"y_onehot\",\n",
        "                )\n",
        "\n",
        "            learning_rate = tf.placeholder(dtype=tf.float32, name=\"learning_rate\")\n",
        "\n",
        "            with tf.name_scope(\"training_ops\"):\n",
        "                with tf.name_scope(\"weights\"):\n",
        "                    weight = tf.get_variable(\n",
        "                        name=\"weights\",\n",
        "                        initializer=tf.random_normal(\n",
        "                            [self.num_features, self.num_classes], stddev=0.01\n",
        "                        ),\n",
        "                    )\n",
        "                    self.variable_summaries(weight)\n",
        "                with tf.name_scope(\"biases\"):\n",
        "                    bias = tf.get_variable(\n",
        "                        name=\"biases\",\n",
        "                        initializer=tf.constant([0.1], shape=[self.num_classes]),\n",
        "                    )\n",
        "                    self.variable_summaries(bias)\n",
        "                with tf.name_scope(\"Wx_plus_b\"):\n",
        "                    output = tf.matmul(x_input, weight) + bias\n",
        "                    tf.summary.histogram(\"pre-activations\", output)\n",
        "\n",
        "            with tf.name_scope(\"svm\"):\n",
        "                regularization = tf.reduce_mean(tf.square(weight))\n",
        "                hinge_loss = tf.reduce_mean(\n",
        "                    tf.square(\n",
        "                        tf.maximum(\n",
        "                            tf.zeros([self.batch_size, self.num_classes]),\n",
        "                            1 - tf.cast(y_onehot, tf.float32) * output,\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "                with tf.name_scope(\"loss\"):\n",
        "                    loss = regularization + self.svm_c * hinge_loss\n",
        "            tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(\n",
        "                loss\n",
        "            )\n",
        "\n",
        "            with tf.name_scope(\"accuracy\"):\n",
        "                predicted_class = tf.sign(output)\n",
        "                predicted_class = tf.identity(predicted_class, name=\"prediction\")\n",
        "                with tf.name_scope(\"correct_prediction\"):\n",
        "                    correct = tf.equal(\n",
        "                        tf.argmax(predicted_class, 1), tf.argmax(y_onehot, 1)\n",
        "                    )\n",
        "                with tf.name_scope(\"accuracy\"):\n",
        "                    accuracy = tf.reduce_mean(tf.cast(correct, \"float\"))\n",
        "            tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "            merged = tf.summary.merge_all()\n",
        "\n",
        "            self.x_input = x_input\n",
        "            self.y_input = y_input\n",
        "            self.y_onehot = y_onehot\n",
        "            self.learning_rate = learning_rate\n",
        "            self.loss = loss\n",
        "            self.optimizer = optimizer\n",
        "            self.output = output\n",
        "            self.predicted_class = predicted_class\n",
        "            self.accuracy = accuracy\n",
        "            self.merged = merged\n",
        "\n",
        "        sys.stdout.write(\"\\n<log> Building graph...\")\n",
        "        __graph__()\n",
        "        sys.stdout.write(\"</log>\\n\")\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        epochs,\n",
        "        log_path,\n",
        "        train_data,\n",
        "        train_size,\n",
        "        validation_data,\n",
        "        validation_size,\n",
        "        result_path,\n",
        "    ):\n",
        "        \"\"\"Trains the SVM model\n",
        "        Parameter\n",
        "        ---------\n",
        "        epochs : int\n",
        "          The number of passes through the entire dataset.\n",
        "        log_path : str\n",
        "          The directory where to save the TensorBoard logs.\n",
        "        train_data : numpy.ndarray\n",
        "          The numpy.ndarray to be used as the training dataset.\n",
        "        train_size : int\n",
        "          The number of data in `train_data`.\n",
        "        validation_data : numpy.ndarray\n",
        "          The numpy.ndarray to be used as the validation dataset.\n",
        "        validation_size : int\n",
        "          The number of data in `validation_data`.\n",
        "        result_path : str\n",
        "          The path where to save the NPY files consisting of the actual and predicted labels.\n",
        "        \"\"\"\n",
        "\n",
        "        # initialize the variables\n",
        "        init_op = tf.group(\n",
        "            tf.global_variables_initializer(), tf.local_variables_initializer()\n",
        "        )\n",
        "\n",
        "        # get the current time and date\n",
        "        timestamp = str(time.asctime())\n",
        "\n",
        "        # event files to contain the TensorBoard log\n",
        "        train_writer = tf.summary.FileWriter(\n",
        "            log_path + timestamp + \"-training\", graph=tf.get_default_graph()\n",
        "        )\n",
        "        test_writer = tf.summary.FileWriter(\n",
        "            os.path.join(log_path, timestamp + \"-testing\"), graph=tf.get_default_graph()\n",
        "        )\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(init_op)\n",
        "\n",
        "            try:\n",
        "                for step in range(epochs * train_size // self.batch_size):\n",
        "                    offset = (step * self.batch_size) % train_size\n",
        "                    batch_train_data = train_data[0][\n",
        "                        offset : (offset + self.batch_size)\n",
        "                    ]\n",
        "                    batch_train_labels = train_data[1][\n",
        "                        offset : (offset + self.batch_size)\n",
        "                    ]\n",
        "\n",
        "                    feed_dict = {\n",
        "                        self.x_input: batch_train_data,\n",
        "                        self.y_input: batch_train_labels,\n",
        "                        self.learning_rate: self.alpha,\n",
        "                    }\n",
        "\n",
        "                    summary, _, step_loss = sess.run(\n",
        "                        [self.merged, self.optimizer, self.loss], feed_dict=feed_dict\n",
        "                    )\n",
        "\n",
        "                    if step % 100 == 0:\n",
        "                        train_accuracy = sess.run(self.accuracy, feed_dict=feed_dict)\n",
        "                        print(\n",
        "                            \"step[{}] train -- loss : {}, accuracy : {}\".format(\n",
        "                                step, step_loss, train_accuracy\n",
        "                            )\n",
        "                        )\n",
        "                        train_writer.add_summary(summary=summary, global_step=step)\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"Training interrupted at step {}\".format(step))\n",
        "                os._exit(1)\n",
        "            finally:\n",
        "                print(\"EOF -- training done at step {}\".format(step))\n",
        "\n",
        "                for step in range(epochs * validation_size // self.batch_size):\n",
        "\n",
        "                    feed_dict = {\n",
        "                        self.x_input: validation_data[0][: self.batch_size],\n",
        "                        self.y_input: validation_data[1][: self.batch_size],\n",
        "                    }\n",
        "\n",
        "                    (\n",
        "                        validation_summary,\n",
        "                        predictions,\n",
        "                        actual,\n",
        "                        validation_loss,\n",
        "                        validation_accuracy,\n",
        "                    ) = sess.run(\n",
        "                        [\n",
        "                            self.merged,\n",
        "                            self.predicted_class,\n",
        "                            self.y_onehot,\n",
        "                            self.loss,\n",
        "                            self.accuracy,\n",
        "                        ],\n",
        "                        feed_dict=feed_dict,\n",
        "                    )\n",
        "\n",
        "                    if step % 100 == 0 and step > 0:\n",
        "                        print(\n",
        "                            \"step [{}] validation -- loss : {}, accuracy : {}\".format(\n",
        "                                step, validation_loss, validation_accuracy\n",
        "                            )\n",
        "                        )\n",
        "                        test_writer.add_summary(\n",
        "                            summary=validation_summary, global_step=step\n",
        "                        )\n",
        "\n",
        "                    self.save_labels(\n",
        "                        predictions=predictions,\n",
        "                        actual=actual,\n",
        "                        result_path=result_path,\n",
        "                        step=step,\n",
        "                        phase=\"testing\",\n",
        "                    )\n",
        "\n",
        "                print(\"EOF -- testing done at step {}\".format(step))\n",
        "\n",
        "    @staticmethod\n",
        "    def variable_summaries(var):\n",
        "        with tf.name_scope(\"summaries\"):\n",
        "            mean = tf.reduce_mean(var)\n",
        "            tf.summary.scalar(\"mean\", mean)\n",
        "            with tf.name_scope(\"stddev\"):\n",
        "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "            tf.summary.scalar(\"stddev\", stddev)\n",
        "            tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
        "            tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
        "            tf.summary.histogram(\"histogram\", var)\n",
        "\n",
        "    @staticmethod\n",
        "    def save_labels(predictions, actual, result_path, step, phase):\n",
        "        \"\"\"Saves the actual and predicted labels to a NPY file\n",
        "        Parameter\n",
        "        ---------\n",
        "        predictions : numpy.ndarray\n",
        "          The NumPy array containing the predicted labels.\n",
        "        actual : numpy.ndarray\n",
        "          The NumPy array containing the actual labels.\n",
        "        result_path : str\n",
        "          The path where to save the concatenated actual and predicted labels.\n",
        "        step : int\n",
        "          The time step for the NumPy arrays.\n",
        "        phase : str\n",
        "          The phase for which the predictions is, i.e. training/validation/testing.\n",
        "        \"\"\"\n",
        "\n",
        "        if not os.path.exists(path=result_path):\n",
        "            os.mkdir(result_path)\n",
        "\n",
        "        # Concatenate the predicted and actual labels\n",
        "        labels = np.concatenate((predictions, actual), axis=1)\n",
        "\n",
        "        # save the labels array to NPY file\n",
        "        np.save(\n",
        "            file=os.path.join(result_path, \"{}-svm-{}.npy\".format(phase, step)),\n",
        "            arr=labels,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o2EqKINOAto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "3e2e8407-7f9d-48a3-afab-bd8b14abffac"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 500\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "\n",
        "class SVM(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SVM, self).__init__()\n",
        "        self.weights_var = tf.Variable(\n",
        "            tf.random.normal(\n",
        "                stddev=1e-2, shape=[kwargs[\"num_features\"], kwargs[\"num_classes\"]]\n",
        "            )\n",
        "        )\n",
        "        self.biases_var = tf.Variable(\n",
        "            tf.random.normal(stddev=1e-2, shape=[kwargs[\"num_classes\"]])\n",
        "        )\n",
        "\n",
        "    def call(self, features):\n",
        "        output = tf.matmul(features, self.weights_var) + self.biases_var\n",
        "        return output\n",
        "\n",
        "\n",
        "def loss_fn(model, features, labels):\n",
        "    output = model(features)\n",
        "    regularization = tf.reduce_mean(model.weights_var)\n",
        "    squared_hinge_loss = tf.reduce_mean(\n",
        "        tf.square(\n",
        "            tf.maximum(\n",
        "                tf.zeros([BATCH_SIZE, NUM_CLASSES]),\n",
        "                (1.0 - tf.cast(labels, tf.float32) * output),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    loss = regularization + 5e-1 * squared_hinge_loss\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train_step(model, opt, loss, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        train_loss = loss(model, features, labels)\n",
        "    gradients = tape.gradient(train_loss, [model.weights_var, model.biases_var])\n",
        "    opt.apply_gradients(zip(gradients, [model.weights_var, model.biases_var]))\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def train(model, opt, loss, dataset, epochs=EPOCHS):\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = []\n",
        "        epoch_accuracy = []\n",
        "        for batch_features, batch_labels in dataset:\n",
        "            train_loss = train_step(model, opt, loss, batch_features, batch_labels)\n",
        "            predictions = model(batch_features)\n",
        "            predictions = tf.sign(predictions)\n",
        "            predictions = predictions.numpy().reshape(-1, NUM_CLASSES)\n",
        "            accuracy = tf.metrics.Accuracy()\n",
        "            accuracy(tf.argmax(predictions, 1), tf.argmax(batch_labels, 1))\n",
        "            epoch_loss.append(train_loss)\n",
        "            epoch_accuracy.append(accuracy.result().numpy())\n",
        "        epoch_loss = tf.reduce_mean(epoch_loss)\n",
        "        epoch_accuracy = tf.reduce_mean(epoch_accuracy)\n",
        "        if epoch != 0 and (epoch + 1) % 100 == 0:\n",
        "            print(\n",
        "                \"epoch {}/{} : mean loss = {}, mean accuracy = {}\".format(\n",
        "                    epoch + 1, epochs, epoch_loss, epoch_accuracy\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "features, labels = load_breast_cancer().data, load_breast_cancer().target\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.10, stratify=labels\n",
        ")\n",
        "\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "y_train[y_train == 0] = -1\n",
        "y_test[y_test == 0] = -1\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.prefetch(BATCH_SIZE * 4)\n",
        "train_dataset = train_dataset.shuffle(BATCH_SIZE * 4)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, True)\n",
        "\n",
        "model = SVM(num_features=30, num_classes=NUM_CLASSES)\n",
        "# optimizer = tf.optimizers.Adam(learning_rate=1e-1, decay=1e-6)\n",
        "optimizer = tf.optimizers.SGD(\n",
        "    learning_rate=1e-1, momentum=9e-1, decay=1e-6, nesterov=True\n",
        ")\n",
        "model(x_train[:10])\n",
        "print(model.summary())\n",
        "train(model, optimizer, loss_fn, train_dataset)\n",
        "model.trainable = False\n",
        "print(model.summary())\n",
        "predictions = model(x_test)\n",
        "predictions = tf.sign(predictions)\n",
        "predictions = predictions.numpy().reshape(-1, NUM_CLASSES)\n",
        "accuracy = tf.metrics.Accuracy()\n",
        "test_accuracy = accuracy(tf.argmax(predictions, 1), tf.argmax(y_test, 1))\n",
        "print(\"test accuracy : {}\".format(test_accuracy.numpy()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"svm_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 62\n",
            "Trainable params: 62\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "epoch 100/500 : mean loss = -23.13077163696289, mean accuracy = 0.880859375\n",
            "epoch 200/500 : mean loss = -46.044715881347656, mean accuracy = 0.849609375\n",
            "epoch 300/500 : mean loss = -68.56888580322266, mean accuracy = 0.8359375\n",
            "epoch 400/500 : mean loss = -90.74737548828125, mean accuracy = 0.830078125\n",
            "epoch 500/500 : mean loss = -112.84288024902344, mean accuracy = 0.84375\n",
            "Model: \"svm_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 62\n",
            "Trainable params: 0\n",
            "Non-trainable params: 62\n",
            "_________________________________________________________________\n",
            "None\n",
            "test accuracy : 0.9649122953414917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvlgIS5IOdn5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d54b592b-7b95-4240-ec05-80da8cf69ea2"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKtc2UN-PJrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49952942-427b-462e-e00a-7013573b1611"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De8FdYPKPWUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}