{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm siamese text similiarity",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvblY8SYvnj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "\n",
        "def train_word2vec(documents, embedding_dim):\n",
        "    \"\"\"\n",
        "    train word2vector over traning documents\n",
        "    Args:\n",
        "        documents (list): list of document\n",
        "        embedding_dim (int): outpu wordvector size\n",
        "    Returns:\n",
        "        word_vectors(dict): dict containing words and their respective vectors\n",
        "    \"\"\"\n",
        "    model = Word2Vec(documents, min_count=1, size=embedding_dim)\n",
        "    word_vectors = model.wv\n",
        "    del model\n",
        "    return word_vectors\n",
        "\n",
        "\n",
        "def create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\n",
        "    \"\"\"\n",
        "    Create embedding matrix containing word indexes and respective vectors from word vectors\n",
        "    Args:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object containing word indexes\n",
        "        word_vectors (dict): dict containing word and their respective vectors\n",
        "        embedding_dim (int): dimention of word vector\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    nb_words = len(tokenizer.word_index) + 1\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
        "    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_vector = word_vectors[word]\n",
        "            if embedding_vector is not None:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            print(\"vector not found for word - %s\" % word)\n",
        "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def word_embed_meta_data(documents, embedding_dim):\n",
        "    \"\"\"\n",
        "    Load tokenizer object for given vocabs list\n",
        "    Args:\n",
        "        documents (list): list of document\n",
        "        embedding_dim (int): embedding dimension\n",
        "    Returns:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
        "        embedding_matrix (dict): dict with word_index and vector mapping\n",
        "    \"\"\"\n",
        "    documents = [x.lower().split() for x in documents]\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(documents)\n",
        "    word_vector = train_word2vec(documents, embedding_dim)\n",
        "    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\n",
        "    del word_vector\n",
        "    gc.collect()\n",
        "    return tokenizer, embedding_matrix\n",
        "\n",
        "\n",
        "def create_train_dev_set(tokenizer, sentences_pair, is_similar, max_sequence_length, validation_split_ratio):\n",
        "    \"\"\"\n",
        "    Create training and validation dataset\n",
        "    Args:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
        "        sentences_pair (list): list of tuple of sentences pairs\n",
        "        is_similar (list): list containing labels if respective sentences in sentence1 and sentence2\n",
        "                           are same or not (1 if same else 0)\n",
        "        max_sequence_length (int): max sequence length of sentences to apply padding\n",
        "        validation_split_ratio (float): contain ratio to split training data into validation data\n",
        "    Returns:\n",
        "        train_data_1 (list): list of input features for training set from sentences1\n",
        "        train_data_2 (list): list of input features for training set from sentences2\n",
        "        labels_train (np.array): array containing similarity score for training data\n",
        "        leaks_train(np.array): array of training leaks features\n",
        "        val_data_1 (list): list of input features for validation set from sentences1\n",
        "        val_data_2 (list): list of input features for validation set from sentences1\n",
        "        labels_val (np.array): array containing similarity score for validation data\n",
        "        leaks_val (np.array): array of validation leaks features\n",
        "    \"\"\"\n",
        "    sentences1 = [x[0].lower() for x in sentences_pair]\n",
        "    sentences2 = [x[1].lower() for x in sentences_pair]\n",
        "    train_sequences_1 = tokenizer.texts_to_sequences(sentences1)\n",
        "    train_sequences_2 = tokenizer.texts_to_sequences(sentences2)\n",
        "    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
        "             for x1, x2 in zip(train_sequences_1, train_sequences_2)]\n",
        "\n",
        "    train_padded_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\n",
        "    train_padded_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\n",
        "    train_labels = np.array(is_similar)\n",
        "    leaks = np.array(leaks)\n",
        "\n",
        "    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\n",
        "    train_data_1_shuffled = train_padded_data_1[shuffle_indices]\n",
        "    train_data_2_shuffled = train_padded_data_2[shuffle_indices]\n",
        "    train_labels_shuffled = train_labels[shuffle_indices]\n",
        "    leaks_shuffled = leaks[shuffle_indices]\n",
        "\n",
        "    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))\n",
        "\n",
        "    del train_padded_data_1\n",
        "    del train_padded_data_2\n",
        "    gc.collect()\n",
        "\n",
        "    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]\n",
        "    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]\n",
        "    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]\n",
        "    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]\n",
        "\n",
        "    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val\n",
        "\n",
        "\n",
        "def create_test_data(tokenizer, test_sentences_pair, max_sequence_length):\n",
        "    \"\"\"\n",
        "    Create training and validation dataset\n",
        "    Args:\n",
        "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
        "        test_sentences_pair (list): list of tuple of sentences pairs\n",
        "        max_sequence_length (int): max sequence length of sentences to apply padding\n",
        "    Returns:\n",
        "        test_data_1 (list): list of input features for training set from sentences1\n",
        "        test_data_2 (list): list of input features for training set from sentences2\n",
        "    \"\"\"\n",
        "    test_sentences1 = [x[0].lower() for x in test_sentences_pair]\n",
        "    test_sentences2 = [x[1].lower() for x in test_sentences_pair]\n",
        "\n",
        "    test_sequences_1 = tokenizer.texts_to_sequences(test_sentences1)\n",
        "    test_sequences_2 = tokenizer.texts_to_sequences(test_sentences2)\n",
        "    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
        "                  for x1, x2 in zip(test_sequences_1, test_sequences_2)]\n",
        "\n",
        "    leaks_test = np.array(leaks_test)\n",
        "    test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\n",
        "    test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\n",
        "\n",
        "    return test_data_1, test_data_2, leaks_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrhKBcawNOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.models import load_model\n",
        "from keras.models import Model\n",
        "\n",
        "# std imports\n",
        "import time\n",
        "import gc\n",
        "import os\n",
        "\n",
        "\n",
        "class SiameseBiLSTM:\n",
        "    def __init__(self, embedding_dim, max_sequence_length, number_lstm, number_dense, rate_drop_lstm, \n",
        "                 rate_drop_dense, hidden_activation, validation_split_ratio):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.number_lstm_units = number_lstm\n",
        "        self.rate_drop_lstm = rate_drop_lstm\n",
        "        self.number_dense_units = number_dense\n",
        "        self.activation_function = hidden_activation\n",
        "        self.rate_drop_dense = rate_drop_dense\n",
        "        self.validation_split_ratio = validation_split_ratio\n",
        "\n",
        "    def train_model(self, sentences_pair, is_similar, embedding_meta_data, model_save_directory='./'):\n",
        "        \"\"\"\n",
        "        Train Siamese network to find similarity between sentences in `sentences_pair`\n",
        "            Steps Involved:\n",
        "                1. Pass the each from sentences_pairs  to bidirectional LSTM encoder.\n",
        "                2. Merge the vectors from LSTM encodes and passed to dense layer.\n",
        "                3. Pass the  dense layer vectors to sigmoid output layer.\n",
        "                4. Use cross entropy loss to train weights\n",
        "        Args:\n",
        "            sentences_pair (list): list of tuple of sentence pairs\n",
        "            is_similar (list): target value 1 if same sentences pair are similar otherwise 0\n",
        "            embedding_meta_data (dict): dict containing tokenizer and word embedding matrix\n",
        "            model_save_directory (str): working directory for where to save models\n",
        "        Returns:\n",
        "            return (best_model_path):  path of best model\n",
        "        \"\"\"\n",
        "        tokenizer, embedding_matrix = embedding_meta_data['tokenizer'], embedding_meta_data['embedding_matrix']\n",
        "\n",
        "        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
        "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, sentences_pair,\n",
        "                                                                               is_similar, self.max_sequence_length,\n",
        "                                                                               self.validation_split_ratio)\n",
        "\n",
        "        if train_data_x1 is None:\n",
        "            print(\"++++ !! Failure: Unable to train model ++++\")\n",
        "            return None\n",
        "\n",
        "        nb_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "        # Creating word embedding layer\n",
        "        embedding_layer = Embedding(nb_words, self.embedding_dim, weights=[embedding_matrix],\n",
        "                                    input_length=self.max_sequence_length, trainable=False)\n",
        "\n",
        "        # Creating LSTM Encoder\n",
        "        lstm_layer = Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n",
        "\n",
        "        # Creating LSTM Encoder layer for First Sentence\n",
        "        sequence_1_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n",
        "        embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
        "        x1 = lstm_layer(embedded_sequences_1)\n",
        "\n",
        "        # Creating LSTM Encoder layer for Second Sentence\n",
        "        sequence_2_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n",
        "        embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
        "        x2 = lstm_layer(embedded_sequences_2)\n",
        "\n",
        "        # Creating leaks input\n",
        "        leaks_input = Input(shape=(leaks_train.shape[1],))\n",
        "        leaks_dense = Dense(int(self.number_dense_units/2), activation=self.activation_function)(leaks_input)\n",
        "\n",
        "        # Merging two LSTM encodes vectors from sentences to\n",
        "        # pass it to dense layer applying dropout and batch normalisation\n",
        "        merged = concatenate([x1, x2, leaks_dense])\n",
        "        merged = BatchNormalization()(merged)\n",
        "        merged = Dropout(self.rate_drop_dense)(merged)\n",
        "        merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n",
        "        merged = BatchNormalization()(merged)\n",
        "        merged = Dropout(self.rate_drop_dense)(merged)\n",
        "        preds = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "        model = Model(inputs=[sequence_1_input, sequence_2_input, leaks_input], outputs=preds)\n",
        "        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "        STAMP = 'lstm_%d_%d_%.2f_%.2f' % (self.number_lstm_units, self.number_dense_units, self.rate_drop_lstm, self.rate_drop_dense)\n",
        "\n",
        "        checkpoint_dir = model_save_directory + 'checkpoints/' + str(int(time.time())) + '/'\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "        bst_model_path = checkpoint_dir + STAMP + '.h5'\n",
        "\n",
        "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
        "\n",
        "        tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n",
        "\n",
        "        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
        "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
        "                  epochs=200, batch_size=64, shuffle=True,\n",
        "                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
        "\n",
        "        return bst_model_path\n",
        "\n",
        "\n",
        "    def update_model(self, saved_model_path, new_sentences_pair, is_similar, embedding_meta_data):\n",
        "        \"\"\"\n",
        "        Update trained siamese model for given new sentences pairs \n",
        "            Steps Involved:\n",
        "                1. Pass the each from sentences from new_sentences_pair to bidirectional LSTM encoder.\n",
        "                2. Merge the vectors from LSTM encodes and passed to dense layer.\n",
        "                3. Pass the  dense layer vectors to sigmoid output layer.\n",
        "                4. Use cross entropy loss to train weights\n",
        "        Args:\n",
        "            model_path (str): model path of already trained siamese model\n",
        "            new_sentences_pair (list): list of tuple of new sentences pairs\n",
        "            is_similar (list): target value 1 if same sentences pair are similar otherwise 0\n",
        "            embedding_meta_data (dict): dict containing tokenizer and word embedding matrix\n",
        "        Returns:\n",
        "            return (best_model_path):  path of best model\n",
        "        \"\"\"\n",
        "        tokenizer = embedding_meta_data['tokenizer']\n",
        "        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
        "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, new_sentences_pair,\n",
        "                                                                               is_similar, self.max_sequence_length,\n",
        "                                                                               self.validation_split_ratio)\n",
        "        model = load_model(saved_model_path)\n",
        "        model_file_name = saved_model_path.split('/')[-1]\n",
        "        new_model_checkpoint_path  = saved_model_path.split('/')[:-2] + str(int(time.time())) + '/' \n",
        "\n",
        "        new_model_path = new_model_checkpoint_path + model_file_name\n",
        "        model_checkpoint = ModelCheckpoint(new_model_checkpoint_path + model_file_name,\n",
        "                                           save_best_only=True, save_weights_only=False)\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "        tensorboard = TensorBoard(log_dir=new_model_checkpoint_path + \"logs/{}\".format(time.time()))\n",
        "\n",
        "        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
        "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
        "                  epochs=50, batch_size=3, shuffle=True,\n",
        "                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n",
        "\n",
        "        return new_model_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_dfv8FDxC2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 25\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 10\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "\n",
        "RATE_DROP_LSTM = 0.17\n",
        "RATE_DROP_DENSE = 0.25\n",
        "NUMBER_LSTM = 50\n",
        "NUMBER_DENSE_UNITS = 50\n",
        "ACTIVATION_FUNCTION = 'relu'\n",
        "\n",
        "\n",
        "siamese_config = {\n",
        "\t'EMBEDDING_DIM': EMBEDDING_DIM,\n",
        "\t'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH,\n",
        "\t'VALIDATION_SPLIT': VALIDATION_SPLIT,\n",
        "\t'RATE_DROP_LSTM': RATE_DROP_LSTM,\n",
        "\t'RATE_DROP_DENSE': RATE_DROP_DENSE,\n",
        "\t'NUMBER_LSTM': NUMBER_LSTM,\n",
        "\t'NUMBER_DENSE_UNITS': NUMBER_DENSE_UNITS,\n",
        "\t'ACTIVATION_FUNCTION': ACTIVATION_FUNCTION\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88XYTzrPwd6C",
        "colab_type": "code",
        "outputId": "3ced805a-910a-4bc5-84de-3193a81bf781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from operator import itemgetter\n",
        "from keras.models import load_model\n",
        "import pandas as pd\n",
        "\n",
        "########################################\n",
        "############ Data Preperation ##########\n",
        "########################################\n",
        "\n",
        "\n",
        "df = pd.read_csv('sample_data.csv')\n",
        "\n",
        "sentences1 = list(df['sentences1'])\n",
        "sentences2 = list(df['sentences2'])\n",
        "is_similar = list(df['is_similar'])\n",
        "del df\n",
        "\n",
        "\n",
        "####################################\n",
        "######## Word Embedding ############\n",
        "####################################\n",
        "\n",
        "\n",
        "# creating word embedding meta data for word embedding \n",
        "tokenizer, embedding_matrix = word_embed_meta_data(sentences1 + sentences2,  siamese_config['EMBEDDING_DIM'])\n",
        "\n",
        "embedding_meta_data = {\n",
        "\t'tokenizer': tokenizer,\n",
        "\t'embedding_matrix': embedding_matrix\n",
        "}\n",
        "\n",
        "## creating sentence pairs\n",
        "sentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\n",
        "del sentences1\n",
        "del sentences2\n",
        "\n",
        "\n",
        "##########################\n",
        "######## Training ########\n",
        "##########################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Configuration(object):\n",
        "    \"\"\"Dump stuff here\"\"\"\n",
        "\n",
        "CONFIG = Configuration()\n",
        "CONFIG.embedding_dim = siamese_config['EMBEDDING_DIM']\n",
        "CONFIG.max_sequence_length = siamese_config['MAX_SEQUENCE_LENGTH']\n",
        "CONFIG.number_lstm_units = siamese_config['NUMBER_LSTM']\n",
        "CONFIG.rate_drop_lstm = siamese_config['RATE_DROP_LSTM']\n",
        "CONFIG.number_dense_units = siamese_config['NUMBER_DENSE_UNITS']\n",
        "CONFIG.activation_function = siamese_config['ACTIVATION_FUNCTION']\n",
        "CONFIG.rate_drop_dense = siamese_config['RATE_DROP_DENSE']\n",
        "CONFIG.validation_split_ratio = siamese_config['VALIDATION_SPLIT']\n",
        "\n",
        "siamese = SiameseBiLSTM(CONFIG.embedding_dim , CONFIG.max_sequence_length, CONFIG.number_lstm_units , CONFIG.number_dense_units, \n",
        "\t\t\t\t\t    CONFIG.rate_drop_lstm, CONFIG.rate_drop_dense, CONFIG.activation_function, CONFIG.validation_split_ratio)\n",
        "\n",
        "best_model_path = siamese.train_model(sentences_pair, is_similar, embedding_meta_data, model_save_directory='./')\n",
        "\n",
        "\n",
        "########################\n",
        "###### Testing #########\n",
        "########################\n",
        "\n",
        "model = load_model(best_model_path)\n",
        "\n",
        "test_sentence_pairs = [('What can make Physics easy to learn?','How can you make physics easy to learn?'),\n",
        "\t\t\t\t\t   ('How many times a day do a clocks hands overlap?','What does it mean that every time I look at the clock the numbers are the same?')]\n",
        "\n",
        "test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n",
        "\n",
        "preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\n",
        "results = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\n",
        "results.sort(key=itemgetter(2), reverse=True)\n",
        "print(results)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (3052, 25)\n",
            "Null word embeddings: 1\n",
            "Train on 400 samples, validate on 99 samples\n",
            "Epoch 1/200\n",
            "320/400 [=======================>......] - ETA: 0s - loss: 0.8475 - acc: 0.4625"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.336988). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "400/400 [==============================] - 3s 8ms/step - loss: 0.8293 - acc: 0.4800 - val_loss: 0.6193 - val_acc: 0.6768\n",
            "Epoch 2/200\n",
            "400/400 [==============================] - 0s 511us/step - loss: 0.7562 - acc: 0.5725 - val_loss: 0.6099 - val_acc: 0.6768\n",
            "Epoch 3/200\n",
            "400/400 [==============================] - 0s 509us/step - loss: 0.7333 - acc: 0.5650 - val_loss: 0.6046 - val_acc: 0.6768\n",
            "Epoch 4/200\n",
            "400/400 [==============================] - 0s 489us/step - loss: 0.6935 - acc: 0.6075 - val_loss: 0.5996 - val_acc: 0.6768\n",
            "Epoch 5/200\n",
            "400/400 [==============================] - 0s 522us/step - loss: 0.6878 - acc: 0.5975 - val_loss: 0.5961 - val_acc: 0.6768\n",
            "Epoch 6/200\n",
            "400/400 [==============================] - 0s 492us/step - loss: 0.7277 - acc: 0.5675 - val_loss: 0.5929 - val_acc: 0.6768\n",
            "Epoch 7/200\n",
            "400/400 [==============================] - 0s 497us/step - loss: 0.5987 - acc: 0.6825 - val_loss: 0.5911 - val_acc: 0.6768\n",
            "Epoch 8/200\n",
            "400/400 [==============================] - 0s 484us/step - loss: 0.6353 - acc: 0.6275 - val_loss: 0.5855 - val_acc: 0.6768\n",
            "Epoch 9/200\n",
            "400/400 [==============================] - 0s 518us/step - loss: 0.6460 - acc: 0.6300 - val_loss: 0.5841 - val_acc: 0.6768\n",
            "Epoch 10/200\n",
            "400/400 [==============================] - 0s 499us/step - loss: 0.6259 - acc: 0.6325 - val_loss: 0.5851 - val_acc: 0.6768\n",
            "Epoch 11/200\n",
            "400/400 [==============================] - 0s 494us/step - loss: 0.6433 - acc: 0.6100 - val_loss: 0.5779 - val_acc: 0.6768\n",
            "Epoch 12/200\n",
            "400/400 [==============================] - 0s 490us/step - loss: 0.6078 - acc: 0.6375 - val_loss: 0.5726 - val_acc: 0.6768\n",
            "Epoch 13/200\n",
            "400/400 [==============================] - 0s 482us/step - loss: 0.5871 - acc: 0.6600 - val_loss: 0.5713 - val_acc: 0.6768\n",
            "Epoch 14/200\n",
            "400/400 [==============================] - 0s 518us/step - loss: 0.6047 - acc: 0.6275 - val_loss: 0.5688 - val_acc: 0.6768\n",
            "Epoch 15/200\n",
            "400/400 [==============================] - 0s 556us/step - loss: 0.6054 - acc: 0.6550 - val_loss: 0.5684 - val_acc: 0.6768\n",
            "Epoch 16/200\n",
            "400/400 [==============================] - 0s 495us/step - loss: 0.6114 - acc: 0.6550 - val_loss: 0.5671 - val_acc: 0.6768\n",
            "Epoch 17/200\n",
            "400/400 [==============================] - 0s 515us/step - loss: 0.5540 - acc: 0.7075 - val_loss: 0.5636 - val_acc: 0.6768\n",
            "Epoch 18/200\n",
            "400/400 [==============================] - 0s 502us/step - loss: 0.5986 - acc: 0.6650 - val_loss: 0.5638 - val_acc: 0.6970\n",
            "Epoch 19/200\n",
            "400/400 [==============================] - 0s 492us/step - loss: 0.5775 - acc: 0.6550 - val_loss: 0.5618 - val_acc: 0.6970\n",
            "Epoch 20/200\n",
            "400/400 [==============================] - 0s 509us/step - loss: 0.5919 - acc: 0.6400 - val_loss: 0.5589 - val_acc: 0.6869\n",
            "Epoch 21/200\n",
            "400/400 [==============================] - 0s 507us/step - loss: 0.6192 - acc: 0.6475 - val_loss: 0.5582 - val_acc: 0.6869\n",
            "Epoch 22/200\n",
            "400/400 [==============================] - 0s 544us/step - loss: 0.5857 - acc: 0.6550 - val_loss: 0.5545 - val_acc: 0.6970\n",
            "Epoch 23/200\n",
            "400/400 [==============================] - 0s 495us/step - loss: 0.5975 - acc: 0.6450 - val_loss: 0.5531 - val_acc: 0.6970\n",
            "Epoch 24/200\n",
            "400/400 [==============================] - 0s 503us/step - loss: 0.5926 - acc: 0.6650 - val_loss: 0.5532 - val_acc: 0.6970\n",
            "Epoch 25/200\n",
            "400/400 [==============================] - 0s 482us/step - loss: 0.5740 - acc: 0.6725 - val_loss: 0.5542 - val_acc: 0.7071\n",
            "Epoch 26/200\n",
            "400/400 [==============================] - 0s 492us/step - loss: 0.5691 - acc: 0.6575 - val_loss: 0.5513 - val_acc: 0.6970\n",
            "Epoch 27/200\n",
            "400/400 [==============================] - 0s 496us/step - loss: 0.5510 - acc: 0.6650 - val_loss: 0.5518 - val_acc: 0.6465\n",
            "Epoch 28/200\n",
            "400/400 [==============================] - 0s 510us/step - loss: 0.5790 - acc: 0.6925 - val_loss: 0.5541 - val_acc: 0.6364\n",
            "Epoch 29/200\n",
            "400/400 [==============================] - 0s 496us/step - loss: 0.5828 - acc: 0.6950 - val_loss: 0.5570 - val_acc: 0.5960\n",
            "2/2 [==============================] - 0s 132ms/step\n",
            "[('What can make Physics easy to learn?', 'How can you make physics easy to learn?', 0.5171977), ('How many times a day do a clocks hands overlap?', 'What does it mean that every time I look at the clock the numbers are the same?', 0.008095311)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phJlkW-qwzFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}